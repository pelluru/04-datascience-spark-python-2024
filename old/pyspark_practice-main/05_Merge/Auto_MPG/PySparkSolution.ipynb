{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPG Cars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction:\n",
    "\n",
    "The following exercise utilizes data from [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Auto+MPG)\n",
    "\n",
    "### Step 1. Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.20:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>cars</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10db2b700>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"cars\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Import the first dataset [cars1](https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/05_Merge/Auto_MPG/cars1.csv) and [cars2](https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/05_Merge/Auto_MPG/cars2.csv).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### Step 3. Assign each to a variable called cars1 and cars2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------------+----------+------+------------+-----+------+--------------------+----+----+----+----+----+\n",
      "| mpg|cylinders|displacement|horsepower|weight|acceleration|model|origin|                 car| _c9|_c10|_c11|_c12|_c13|\n",
      "+----+---------+------------+----------+------+------------+-----+------+--------------------+----+----+----+----+----+\n",
      "|18.0|        8|         307|       130|  3504|        12.0|   70|     1|chevrolet chevell...|null|null|null|null|null|\n",
      "|15.0|        8|         350|       165|  3693|        11.5|   70|     1|   buick skylark 320|null|null|null|null|null|\n",
      "+----+---------+------------+----------+------+------------+-----+------+--------------------+----+----+----+----+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cars1 = spark.read.csv(\"cars1.csv\", inferSchema=True, header=True, sep=\",\")\n",
    "cars1.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------------+----------+------+------------+-----+------+--------------+\n",
      "| mpg|cylinders|displacement|horsepower|weight|acceleration|model|origin|           car|\n",
      "+----+---------+------------+----------+------+------------+-----+------+--------------+\n",
      "|33.0|        4|          91|        53|  1795|        17.4|   76|     3|   honda civic|\n",
      "|20.0|        6|         225|       100|  3651|        17.7|   76|     1|dodge aspen se|\n",
      "+----+---------+------------+----------+------+------------+-----+------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cars2 = spark.read.csv(\"cars2.csv\", inferSchema=True, header=True, sep=\",\")\n",
    "cars2.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Oops, it seems our first dataset has some unnamed blank columns, fix cars1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model', 'origin', 'car', '_c9', '_c10', '_c11', '_c12', '_c13']\n",
      "['_c9', '_c10', '_c11', '_c12', '_c13']\n"
     ]
    }
   ],
   "source": [
    "print(cars1.columns)\n",
    "\n",
    "cols_to_delete = []\n",
    "for c in cars1.columns:\n",
    "    if c.startswith('_'):\n",
    "        cols_to_delete.append(c)\n",
    "\n",
    "print(cols_to_delete)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------------+----------+------+------------+-----+------+--------------------+\n",
      "| mpg|cylinders|displacement|horsepower|weight|acceleration|model|origin|                 car|\n",
      "+----+---------+------------+----------+------+------------+-----+------+--------------------+\n",
      "|18.0|        8|         307|       130|  3504|        12.0|   70|     1|chevrolet chevell...|\n",
      "|15.0|        8|         350|       165|  3693|        11.5|   70|     1|   buick skylark 320|\n",
      "+----+---------+------------+----------+------+------------+-----+------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cars1 = cars1.drop(*cols_to_delete)\n",
    "cars1.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. What is the number of observations in each dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cars1: 198 , 9\n",
      "cars2: 200 , 9\n"
     ]
    }
   ],
   "source": [
    "print(\"cars1:\", cars1.count(),\",\", len(cars1.columns))\n",
    "print(\"cars2:\", cars2.count(),\",\", len(cars2.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6. Join cars1 and cars2 into a single DataFrame called cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model', 'origin', 'car']\n",
      "['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model', 'origin', 'car']\n"
     ]
    }
   ],
   "source": [
    "print(cars1.columns)\n",
    "print(cars2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------------+----------+------+------------+-----+------+--------------------+\n",
      "| mpg|cylinders|displacement|horsepower|weight|acceleration|model|origin|                 car|\n",
      "+----+---------+------------+----------+------+------------+-----+------+--------------------+\n",
      "|18.0|        8|         307|       130|  3504|        12.0|   70|     1|chevrolet chevell...|\n",
      "|15.0|        8|         350|       165|  3693|        11.5|   70|     1|   buick skylark 320|\n",
      "+----+---------+------------+----------+------+------------+-----+------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The difference between this function and :func:`union` is that this function\n",
    "# resolves columns by name (not by position)\n",
    "cars = cars1.unionByName(cars2)\n",
    "cars.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "398"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7. Oops, there is a column missing, called owners. Create a random number Series from 15,000 to 73,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import numpy as np\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "nr_owners = list(np.random.randint(15000, high=73001, size=398, dtype = 'int'))\n",
    "print(type(nr_owners))\n",
    "print(type(nr_owners[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'list'>\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "nr_owners = list(np.random.randint(15000, high=73001, size=398, dtype = 'int'))\n",
    "print(type(nr_owners))\n",
    "print(type(nr_owners[0]))\n",
    "\n",
    "# list(map(int, ['1','2','3'])) # => [1,2,3]\n",
    "\n",
    "nr_owners = list(map(int, nr_owners))\n",
    "print(type(nr_owners))\n",
    "print(type(nr_owners[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "nr_owners1= list(map(int, list(np.random.randint(15000, high=73001, size=398, dtype = 'int'))))\n",
    "print(type(nr_owners1))\n",
    "print(type(nr_owners1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|owners|\n",
      "+------+\n",
      "| 35821|\n",
      "| 67889|\n",
      "| 46770|\n",
      "| 68824|\n",
      "| 52944|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# schema = [\"owners\"]\n",
    "owners_df = spark.createDataFrame(nr_owners,IntegerType())\n",
    "owners_df = owners_df.withColumnRenamed(\"value\",\"owners\")\n",
    "owners_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8. Add the column owners to cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying to find an answer"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
