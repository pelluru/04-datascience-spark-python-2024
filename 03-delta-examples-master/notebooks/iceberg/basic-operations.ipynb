{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0dabf1f-038a-4642-ac2d-73bc4b1a6726",
   "metadata": {},
   "source": [
    "# Convert Iceberg to Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a1a40f3-0bd6-4eda-b1e7-f92e24cce3df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/matthew.powers/opt/miniconda3/envs/pyspark-332-delta-230/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/matthew.powers/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/matthew.powers/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      "io.delta#delta-iceberg_2.12 added as a dependency\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.3_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-195c9a13-2cef-461f-b9eb-237bd9058758;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.3.0 in central\n",
      "\tfound io.delta#delta-storage;2.3.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound io.delta#delta-iceberg_2.12;2.3.0 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.1.1 in central\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.3_2.12;1.0.0 in central\n",
      ":: resolution report :: resolve 147ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.3.0 from central in [default]\n",
      "\tio.delta#delta-iceberg_2.12;2.3.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.3.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.3_2.12;1.0.0 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.1.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   6   |   0   |   0   |   0   ||   6   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-195c9a13-2cef-461f-b9eb-237bd9058758\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 6 already retrieved (0kB/4ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/07/17 11:06:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import shutil\n",
    "\n",
    "from delta import *\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "testRoot = \"/tmp/delta-iceberg-converter/\"\n",
    "warehousePath = testRoot + \"iceberg_tables\"\n",
    "shutil.rmtree(testRoot, ignore_errors=True)\n",
    "\n",
    "table = \"local.some_db.my_fun_table\"\n",
    "tablePath = \"file://\" + warehousePath + \"/db/table\"\n",
    "\n",
    "builder = (\n",
    "    SparkSession.builder.master(\"local[*]\")\n",
    "    .appName(\"test\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.local.type\", \"hadoop\")\n",
    "    .config(\"spark.sql.catalog.local.warehouse\", warehousePath)\n",
    ")\n",
    "\n",
    "my_packages = [\n",
    "    \"io.delta:delta-iceberg_2.12:2.3.0\",\n",
    "    \"org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.0.0\",\n",
    "]\n",
    "\n",
    "spark = configure_spark_with_delta_pip(\n",
    "    builder, extra_packages=my_packages\n",
    ").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf29580d-553c-4652-af51-0bc50a763951",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://qtk9h72yp0.lan:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10e4523d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93065ec-d2c1-4709-b6a8-d1d6dae7b6bd",
   "metadata": {},
   "source": [
    "## Create Iceberg table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45cb9065-f48f-4804-a79a-69020f46cf6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE local.some_db.my_fun_table (id BIGINT, data STRING) USING ICEBERG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7678a81-6ca4-4032-8f9a-55d99340f931",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"INSERT INTO local.some_db.my_fun_table VALUES (1, 'a'), (2, 'b'), (3, 'c')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54450e2b-b5f0-4d50-ba41-ae2d67b754cd",
   "metadata": {},
   "source": [
    "## Append rows to existing table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8d013dc-c1fb-4f45-999c-2955d25a26c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"INSERT INTO local.some_db.my_fun_table VALUES (4, 'd')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6900144-02a7-41b4-bf15-13e564051ba1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|data|\n",
      "+---+----+\n",
      "|  1|   a|\n",
      "|  4|   d|\n",
      "|  2|   b|\n",
      "|  3|   c|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from local.some_db.my_fun_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "905bd89a-6922-4fa8-ba06-af463353d904",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/tmp/delta-iceberg-converter/iceberg_tables/some_db/my_fun_table/\u001b[0m\n",
      "├── \u001b[01;34mdata\u001b[0m\n",
      "│   ├── \u001b[00m00000-0-ba967636-9bc4-4624-af57-c8eecf6e93ff-00001.parquet\u001b[0m\n",
      "│   ├── \u001b[00m00000-3-55d19b49-b2b4-43a6-b6bb-b70573a1f39d-00001.parquet\u001b[0m\n",
      "│   ├── \u001b[00m00001-1-270a4c8f-eb78-412d-b383-6a8ba380f97c-00001.parquet\u001b[0m\n",
      "│   └── \u001b[00m00002-2-4167155a-f76a-48ff-8f6c-c57d16d17404-00001.parquet\u001b[0m\n",
      "└── \u001b[01;34mmetadata\u001b[0m\n",
      "    ├── \u001b[00m3f5d7388-055d-41ac-a81f-b6238698dfae-m0.avro\u001b[0m\n",
      "    ├── \u001b[00m53857548-893b-41af-b5ef-331660534303-m0.avro\u001b[0m\n",
      "    ├── \u001b[00msnap-1156653990101004919-1-53857548-893b-41af-b5ef-331660534303.avro\u001b[0m\n",
      "    ├── \u001b[00msnap-3920219495340820176-1-3f5d7388-055d-41ac-a81f-b6238698dfae.avro\u001b[0m\n",
      "    ├── \u001b[00mv1.metadata.json\u001b[0m\n",
      "    ├── \u001b[00mv2.metadata.json\u001b[0m\n",
      "    ├── \u001b[00mv3.metadata.json\u001b[0m\n",
      "    └── \u001b[00mversion-hint.text\u001b[0m\n",
      "\n",
      "2 directories, 12 files\n"
     ]
    }
   ],
   "source": [
    "!tree /tmp/delta-iceberg-converter/iceberg_tables/some_db/my_fun_table/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64196765-6854-4bdc-9a01-b65edaa34628",
   "metadata": {},
   "source": [
    "## Alter table ADD COLUMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3869642-dd1e-4308-b5c6-757d02cc19d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "ALTER TABLE local.some_db.my_fun_table\n",
    "ADD COLUMNS (\n",
    "    my_new_column string\n",
    "  )\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff845a68-dc6a-43bc-a358-d4657c7b8bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------------+\n",
      "| id|data|my_new_column|\n",
      "+---+----+-------------+\n",
      "|  1|   a|         null|\n",
      "|  4|   d|         null|\n",
      "|  2|   b|         null|\n",
      "|  3|   c|         null|\n",
      "+---+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from local.some_db.my_fun_table\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d119e9fc-cab3-412c-a4d8-937295906135",
   "metadata": {},
   "source": [
    "## Alter table RENAME COLUMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0d82fc4-1fd5-4b0d-81c0-71582f813dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "ALTER TABLE local.some_db.my_fun_table RENAME COLUMN data TO letter\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06a6d8dd-8548-4a23-a335-7c7a2156794c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------------+\n",
      "| id|letter|my_new_column|\n",
      "+---+------+-------------+\n",
      "|  4|     d|         null|\n",
      "|  1|     a|         null|\n",
      "|  2|     b|         null|\n",
      "|  3|     c|         null|\n",
      "+---+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from local.some_db.my_fun_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f736b6-7851-415d-814d-d1dff3f6f2d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04e7a7e0-a042-41db-99d8-094f079adde7",
   "metadata": {},
   "source": [
    "## Drop table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5bb736-36de-420a-ae71-bcd236964480",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95a07fde-563d-4482-a511-542d157269e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c000ed9c-eda4-4451-871a-d371f915791b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shutil.rmtree(testRoot, ignore_errors=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark-332-delta-230]",
   "language": "python",
   "name": "conda-env-pyspark-332-delta-230-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
