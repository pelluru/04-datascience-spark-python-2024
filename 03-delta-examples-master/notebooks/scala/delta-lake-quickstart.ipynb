{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d16db84-18af-4b76-a604-f128bb86d45d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.2.2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5cd9066-9bf8-4214-8b08-5758daea8735",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                           \u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`io.delta::delta-core:2.0.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dd5df17-9f5b-49ab-a089-35b0de183764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mio.delta._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mio.delta.tables._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io.delta._\n",
    "import io.delta.tables._\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecd224a3-88b7-4756-895a-30be1276caf0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "22/08/08 12:33:34 INFO SparkContext: Running Spark version 3.2.2\n",
      "22/08/08 12:33:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/08/08 12:33:34 INFO ResourceUtils: ==============================================================\n",
      "22/08/08 12:33:34 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "22/08/08 12:33:34 INFO ResourceUtils: ==============================================================\n",
      "22/08/08 12:33:34 INFO SparkContext: Submitted application: test\n",
      "22/08/08 12:33:34 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "22/08/08 12:33:34 INFO ResourceProfile: Limiting resource is cpu\n",
      "22/08/08 12:33:34 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "22/08/08 12:33:34 INFO SecurityManager: Changing view acls to: matthew.powers\n",
      "22/08/08 12:33:34 INFO SecurityManager: Changing modify acls to: matthew.powers\n",
      "22/08/08 12:33:34 INFO SecurityManager: Changing view acls groups to: \n",
      "22/08/08 12:33:34 INFO SecurityManager: Changing modify acls groups to: \n",
      "22/08/08 12:33:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(matthew.powers); groups with view permissions: Set(); users  with modify permissions: Set(matthew.powers); groups with modify permissions: Set()\n",
      "22/08/08 12:33:34 INFO Utils: Successfully started service 'sparkDriver' on port 57702.\n",
      "22/08/08 12:33:35 INFO SparkEnv: Registering MapOutputTracker\n",
      "22/08/08 12:33:35 INFO SparkEnv: Registering BlockManagerMaster\n",
      "22/08/08 12:33:35 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "22/08/08 12:33:35 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "22/08/08 12:33:35 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/08/08 12:33:35 INFO DiskBlockManager: Created local directory at /private/var/folders/19/_52w4zps3xjc6plz_f63j8sh0000gp/T/blockmgr-8cb6538e-7ecc-4335-9d57-e628b75b2a2a\n",
      "22/08/08 12:33:35 INFO MemoryStore: MemoryStore started with capacity 8.4 GiB\n",
      "22/08/08 12:33:35 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "22/08/08 12:33:35 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "22/08/08 12:33:35 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://qtk9h72yp0:4040\n",
      "22/08/08 12:33:36 INFO Executor: Starting executor ID driver on host qtk9h72yp0\n",
      "22/08/08 12:33:36 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57703.\n",
      "22/08/08 12:33:36 INFO NettyBlockTransferService: Server created on qtk9h72yp0:57703\n",
      "22/08/08 12:33:36 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "22/08/08 12:33:36 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, qtk9h72yp0, 57703, None)\n",
      "22/08/08 12:33:36 INFO BlockManagerMasterEndpoint: Registering block manager qtk9h72yp0:57703 with 8.4 GiB RAM, BlockManagerId(driver, qtk9h72yp0, 57703, None)\n",
      "22/08/08 12:33:36 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, qtk9h72yp0, 57703, None)\n",
      "22/08/08 12:33:36 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, qtk9h72yp0, 57703, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@74ec78ee"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = org.apache.spark.sql.SparkSession.builder()\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .appName(\"test\").master(\"local\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70e286cc-8d60-4fbc-a879-9404a027b936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdata\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mDataset\u001b[39m[\u001b[32mjava\u001b[39m.\u001b[32mlang\u001b[39m.\u001b[32mLong\u001b[39m] = [id: bigint]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = spark.range(0, 5)\n",
    "data.write.format(\"delta\").save(\"tmp/delta-table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae367c7-95f5-4a8b-b560-4f7e7dbf36d8",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82bb552f-f8c8-4521-a494-a701170ecb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [id: bigint]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.format(\"delta\").load(\"tmp/delta-table\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3a3342-bcfb-4091-84e0-f45d8ce68774",
   "metadata": {},
   "source": [
    "## Update table data: overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39878d70-91f7-428e-a5e8-2c6afb738843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdata\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mDataset\u001b[39m[\u001b[32mjava\u001b[39m.\u001b[32mlang\u001b[39m.\u001b[32mLong\u001b[39m] = [id: bigint]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = spark.range(5, 10)\n",
    "data.write.format(\"delta\").mode(\"overwrite\").save(\"tmp/delta-table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f963ea2b-af37-486d-9f9b-558ffc1abc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1527dac-3037-450d-920c-de981d61ebb4",
   "metadata": {},
   "source": [
    "## Update table data: conditional update without overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fce90bf1-4970-4bbc-ba51-a2883f39923b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdeltaTable\u001b[39m: \u001b[32mDeltaTable\u001b[39m = io.delta.tables.DeltaTable@37b9b9ab"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val deltaTable = DeltaTable.forPath(\"tmp/delta-table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6237e95b-6741-47e2-813a-2c26da233954",
   "metadata": {},
   "source": [
    "Update every even value by adding 100 to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f29275cf-14b9-4d04-99cd-e42c0d2ecabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable.update(\n",
    "  condition = expr(\"id % 2 == 0\"),\n",
    "  set = Map(\"id\" -> expr(\"id + 100\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03a6e35e-15c6-48b4-a54b-9145cd4509e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  5|\n",
      "|106|\n",
      "|  7|\n",
      "|108|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaTable.toDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e65aeeb-3ffc-4f1c-b4f5-72beab700757",
   "metadata": {},
   "source": [
    "Delete every even value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cbe43d0-5dd4-4992-b271-5c520b89dfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable.delete(condition = expr(\"id % 2 == 0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3cbdf87-c4a4-4d0e-8dd6-0ed302cd1f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  5|\n",
      "|  7|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaTable.toDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1e04cc-c725-4343-8601-3f9b7c352720",
   "metadata": {},
   "source": [
    "Upsert (merge) new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79973d5b-2887-4438-8b65-de760c1b170a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mnewData\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [id: bigint]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newData = spark.range(0, 20).toDF\n",
    "\n",
    "deltaTable.as(\"oldData\")\n",
    "  .merge(\n",
    "    newData.as(\"newData\"),\n",
    "    \"oldData.id = newData.id\")\n",
    "  .whenMatched\n",
    "  .update(Map(\"id\" -> col(\"newData.id\")))\n",
    "  .whenNotMatched\n",
    "  .insert(Map(\"id\" -> col(\"newData.id\")))\n",
    "  .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b41c4bb5-b090-4bd9-acb3-7cb6f5f75c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaTable.toDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e538c593-6484-40d6-84ff-4fc9038cbd3d",
   "metadata": {},
   "source": [
    "## Read older versions of data with time travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b999b26a-7e5f-45bd-8376-5dad3b6b49cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [id: bigint]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"tmp/delta-table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24ade984-00fb-4b97-9e7f-e3da1cb57714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
