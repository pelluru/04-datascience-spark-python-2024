{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc26d5dd-5c02-4527-9411-be2bd4946c09",
   "metadata": {},
   "source": [
    "# Convert CSV to Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f44f206-afde-4818-9ce8-6b2bdf07d189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import delta\n",
    "import pyspark\n",
    "from delta import configure_spark_with_delta_pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "295791ac-bf05-4fa6-a6ba-accc5a66aa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = (\n",
    "    pyspark.sql.SparkSession.builder.appName(\"MyApp\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55b2f916-6279-4bf6-be7d-dff7c5b0566a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/matthew.powers/opt/miniconda3/envs/pyspark-330-delta-210/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/matthew.powers/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/matthew.powers/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-cd133e32-face-4ff2-8dd6-a8f6dfcbb4c4;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.1.0 in central\n",
      "\tfound io.delta#delta-storage;2.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      ":: resolution report :: resolve 115ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.1.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-cd133e32-face-4ff2-8dd6-a8f6dfcbb4c4\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/4ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/18 13:45:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/18 13:45:18 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/03/18 13:45:18 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a3cd11-368c-49a3-99aa-c351ff1908c8",
   "metadata": {},
   "source": [
    "## Convert CSV to Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6c05daa-5e04-4bb3-a05d-247d8d2fc545",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", True).load(\"../../data/students/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65c3eac0-6adb-4add-b29d-a1e812bf73cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+-------+\n",
      "|student_name|graduation_year|  major|\n",
      "+------------+---------------+-------+\n",
      "| chrisXXborg|           2025|    bio|\n",
      "|davidXXcross|           2026|physics|\n",
      "|sophiaXXraul|           2022|    bio|\n",
      "|    fredXXli|           2025|physics|\n",
      "|someXXperson|           2023|   math|\n",
      "|     liXXyao|           2025|physics|\n",
      "+------------+---------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d44df4c-91b5-4bea-b440-28bbbc411e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.format(\"delta\").save(\"tmp/students_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07dc877c-bc22-43d9-bc3d-6fa12a4e8d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mtmp/students_delta\u001b[0m\n",
      "├── \u001b[01;34m_delta_log\u001b[0m\n",
      "│   └── \u001b[00m00000000000000000000.json\u001b[0m\n",
      "├── \u001b[00mpart-00000-f88eff9c-a087-4354-8c04-4eb3da62a10e-c000.snappy.parquet\u001b[0m\n",
      "├── \u001b[00mpart-00001-0bc69839-449f-4a64-87b6-91e28bfb7423-c000.snappy.parquet\u001b[0m\n",
      "└── \u001b[00mpart-00002-a093e875-4eb5-4372-8b4f-def98ec208a6-c000.snappy.parquet\u001b[0m\n",
      "\n",
      "1 directory, 4 files\n"
     ]
    }
   ],
   "source": [
    "!tree tmp/students_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88a19405-1773-448e-8b21-2285af96e745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+-------+\n",
      "|student_name|graduation_year|  major|\n",
      "+------------+---------------+-------+\n",
      "| chrisXXborg|           2025|    bio|\n",
      "|davidXXcross|           2026|physics|\n",
      "|sophiaXXraul|           2022|    bio|\n",
      "|    fredXXli|           2025|physics|\n",
      "|someXXperson|           2023|   math|\n",
      "|     liXXyao|           2025|physics|\n",
      "+------------+---------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(\"tmp/students_delta\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a797d7-7def-45a4-b7a9-4cd02ed79bb4",
   "metadata": {},
   "source": [
    "## Clean data before creating Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "884c1ad9-db02-44e1-92af-b4ef36a1ff6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fcca18b-eed1-430c-b556-09f2f0255434",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+-------+\n",
      "|student_name|graduation_year|  major|\n",
      "+------------+---------------+-------+\n",
      "| chrisXXborg|           2025|    bio|\n",
      "|davidXXcross|           2026|physics|\n",
      "|sophiaXXraul|           2022|    bio|\n",
      "|    fredXXli|           2025|physics|\n",
      "|someXXperson|           2023|   math|\n",
      "|     liXXyao|           2025|physics|\n",
      "+------------+---------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5b016ef-3ccf-482e-b956-44319e3e8623",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_df = (\n",
    "    df.withColumn(\"student_first_name\", split(col(\"student_name\"), \"XX\").getItem(0))\n",
    "    .withColumn(\"student_last_name\", split(col(\"student_name\"), \"XX\").getItem(1))\n",
    "    .drop(\"student_name\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c927fbc2-c40b-4685-8f84-0e909e4e2f0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+------------------+-----------------+\n",
      "|graduation_year|  major|student_first_name|student_last_name|\n",
      "+---------------+-------+------------------+-----------------+\n",
      "|           2025|    bio|             chris|             borg|\n",
      "|           2026|physics|             david|            cross|\n",
      "|           2022|    bio|            sophia|             raul|\n",
      "|           2025|physics|              fred|               li|\n",
      "|           2023|   math|              some|           person|\n",
      "|           2025|physics|                li|              yao|\n",
      "+---------------+-------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc519b88-9195-426d-ab9e-51ae90e75ba7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_df.write.format(\"delta\").save(\"tmp/clean_students_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7fe68675-2606-4f1d-88d6-9535ef8b9a03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+------------------+-----------------+\n",
      "|graduation_year|  major|student_first_name|student_last_name|\n",
      "+---------------+-------+------------------+-----------------+\n",
      "|           2025|    bio|             chris|             borg|\n",
      "|           2026|physics|             david|            cross|\n",
      "|           2022|    bio|            sophia|             raul|\n",
      "|           2025|physics|              fred|               li|\n",
      "|           2023|   math|              some|           person|\n",
      "|           2025|physics|                li|              yao|\n",
      "+---------------+-------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(\"tmp/clean_students_delta\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c843f52-5a05-43e3-8fd2-0f07fb09f45b",
   "metadata": {},
   "source": [
    "## Schema enforcement with CSV is bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd1d5e05-b8e6-4496-ad00-c263b09e8d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatched_df = spark.range(0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53233ac4-15f7-4f7b-bf17-fd493f208ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mismatched_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bb96165-3f5c-4265-95e4-ff98286cafc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatched_df.repartition(1).write.mode(\"append\").format(\"csv\").option(\n",
    "    \"header\", True\n",
    ").save(\"../../data/students\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73ec913d-c59e-487a-900e-2ef16af5f987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/24 16:40:46 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 1, schema size: 3\n",
      "CSV file: file:///Users/matthew.powers/Documents/code/my_apps/delta-examples/data/students/part-00000-988a286d-a024-4612-8b6e-89cce5f2556e-c000.csv\n",
      "+------------+---------------+-------+\n",
      "|student_name|graduation_year|  major|\n",
      "+------------+---------------+-------+\n",
      "| chrisXXborg|           2025|    bio|\n",
      "|davidXXcross|           2026|physics|\n",
      "|sophiaXXraul|           2022|    bio|\n",
      "|    fredXXli|           2025|physics|\n",
      "|someXXperson|           2023|   math|\n",
      "|     liXXyao|           2025|physics|\n",
      "|           0|           null|   null|\n",
      "|           1|           null|   null|\n",
      "|           2|           null|   null|\n",
      "+------------+---------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"csv\").option(\"header\", True).load(\"../../data/students/*.csv\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6500d78-8fbd-4d8a-96a8-cf2802e7342e",
   "metadata": {},
   "source": [
    "## Schema enforcement with Delta Lake is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99b2d355-ea68-4a1d-83bc-ebf74b53b6ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "A schema mismatch detected when writing to the Delta table (Table ID: 740d4bb1-d539-4d56-911e-18a616a37940).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- student_name: string (nullable = true)\n-- graduation_year: string (nullable = true)\n-- major: string (nullable = true)\n\n\nData schema:\nroot\n-- id: long (nullable = true)\n\n         ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmismatched_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepartition\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtmp/students_delta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/pyspark-330-delta-210/lib/python3.9/site-packages/pyspark/sql/readwriter.py:968\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 968\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/pyspark-330-delta-210/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/pyspark-330-delta-210/lib/python3.9/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: A schema mismatch detected when writing to the Delta table (Table ID: 740d4bb1-d539-4d56-911e-18a616a37940).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- student_name: string (nullable = true)\n-- graduation_year: string (nullable = true)\n-- major: string (nullable = true)\n\n\nData schema:\nroot\n-- id: long (nullable = true)\n\n         "
     ]
    }
   ],
   "source": [
    "mismatched_df.repartition(1).write.mode(\"append\").format(\"delta\").save(\n",
    "    \"tmp/students_delta\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129a446b-c5eb-4f69-8269-25cf1427ed8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark-330-delta-210]",
   "language": "python",
   "name": "conda-env-pyspark-330-delta-210-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
