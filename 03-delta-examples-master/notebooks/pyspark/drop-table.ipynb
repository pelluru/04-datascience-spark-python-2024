{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b86f537-9e3a-4e06-b6a9-27a8d7841e02",
   "metadata": {},
   "source": [
    "# Drop Delta Tables\n",
    "\n",
    "Dropping Delta Tables is different depending on if the table is managed or unmanaged.  It's important to understand the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eaef3c-32c5-446e-baf9-48f9b01f3a9c",
   "metadata": {},
   "source": [
    "## Unmanaged tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b79a9aa7-e8bc-4b0d-9b90-cd56ffe46d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from delta import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "061d03db-799a-4ebd-89a8-d0df582855cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = (\n",
    "    pyspark.sql.SparkSession.builder.appName(\"MyApp\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca749419-e3fe-491b-a70a-f089f1aa8ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/powers/opt/miniconda3/envs/mr-delta/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/powers/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/powers/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-7a1addab-6f7c-4706-82d6-10ef0d01616b;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.0.0 in central\n",
      "\tfound io.delta#delta-storage;2.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      ":: resolution report :: resolve 316ms :: artifacts dl 20ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-7a1addab-6f7c-4706-82d6-10ef0d01616b\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/8ms)\n",
      "22/07/30 17:25:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d91b7d50-fa68-4286-925d-f068510cf80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cSchema = StructType([StructField(\"items\", StringType())\\\n",
    "                      ,StructField(\"number\", IntegerType())])\n",
    "\n",
    "test_list = [['furniture', 1], ['games', 3]]\n",
    "\n",
    "df = spark.createDataFrame(test_list,schema=cSchema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9ed4eb-0321-4bae-bf42-fd00a18f03c9",
   "metadata": {},
   "source": [
    "## Remove files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc5d23c0-6fd6-49ad-8753-e981275cc113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"tmp/test_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "535bbc3f-74e6-4fab-b804-b35f79fe5660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/powers/opt/miniconda3/envs/mr-delta/lib/python3.9/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/powers/opt/miniconda3/envs/mr-delta/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/powers/opt/miniconda3/envs/mr-delta/lib/python3.9/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mtmp/test_table\u001b[0m\n",
      "├── \u001b[01;34m_delta_log\u001b[0m\n",
      "│   └── \u001b[00m00000000000000000000.json\u001b[0m\n",
      "├── \u001b[00mpart-00000-00f7a436-6d1c-40fb-9b89-a358e7ed3a3a-c000.snappy.parquet\u001b[0m\n",
      "├── \u001b[00mpart-00003-6f09df03-ae51-4c07-a565-a0a1738ee354-c000.snappy.parquet\u001b[0m\n",
      "└── \u001b[00mpart-00007-890d5137-1867-44f4-97c6-cef599392dce-c000.snappy.parquet\u001b[0m\n",
      "\n",
      "1 directory, 4 files\n"
     ]
    }
   ],
   "source": [
    "!tree tmp/test_table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3e01a1e-a1c9-46ce-8e01-807bc9042485",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2ceea2b-8c73-4dac-95b3-0522cdb58f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp/test_table  [error opening dir]\n",
      "\n",
      "0 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "!tree tmp/test_table "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6e84f4-a194-482a-97a0-d811b7418fcb",
   "metadata": {},
   "source": [
    "## Remove managed table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b29713f4-c964-4007-b66e-6a6c277f0c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.saveAsTable(\"your_managed_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0bfa655-e3f0-4a3e-b76d-fa8169d386af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|    items|number|\n",
      "+---------+------+\n",
      "|furniture|     1|\n",
      "|    games|     3|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from your_managed_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62250535-daba-4a4a-b3ee-e1adfb3bca00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mspark-warehouse\u001b[0m\n",
      "└── \u001b[01;34myour_managed_table\u001b[0m\n",
      "    ├── \u001b[00m_SUCCESS\u001b[0m\n",
      "    ├── \u001b[00mpart-00000-21f03761-1492-47eb-a6d6-d064a21a89d3-c000.snappy.parquet\u001b[0m\n",
      "    ├── \u001b[00mpart-00003-21f03761-1492-47eb-a6d6-d064a21a89d3-c000.snappy.parquet\u001b[0m\n",
      "    └── \u001b[00mpart-00007-21f03761-1492-47eb-a6d6-d064a21a89d3-c000.snappy.parquet\u001b[0m\n",
      "\n",
      "1 directory, 4 files\n"
     ]
    }
   ],
   "source": [
    "!tree spark-warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14d9929c-f499-4312-ae10-f605243573d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+-----------+\n",
      "|namespace|         tableName|isTemporary|\n",
      "+---------+------------------+-----------+\n",
      "|  default|your_managed_table|      false|\n",
      "+---------+------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe8606aa-0306-4fb1-b33a-51fb27d57a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"drop table if exists your_managed_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ca349da-851b-4ac4-8728-3aefc1eeeaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b44caa12-2711-43cf-a52c-c3f844b74e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mspark-warehouse\u001b[0m\n",
      "\n",
      "0 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "!tree spark-warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ed36bb-42e6-4b7c-afac-3f3905199ea1",
   "metadata": {},
   "source": [
    "## Removed unmanaged table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ae9548f-9037-4149-9605-a2196e584268",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.option(\"path\", \"tmp/unmanaged_data\").saveAsTable(\"your_unmanaged_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d226c2a-8cfc-46d1-965d-f90b7d5dd2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------+\n",
      "|namespace|           tableName|isTemporary|\n",
      "+---------+--------------------+-----------+\n",
      "|  default|your_unmanaged_table|      false|\n",
      "+---------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d5556e8-9152-45af-a4fd-b18a6f5cf220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mtmp/\u001b[0m\n",
      "└── \u001b[01;34munmanaged_data\u001b[0m\n",
      "    ├── \u001b[00m_SUCCESS\u001b[0m\n",
      "    ├── \u001b[00mpart-00000-ab42755f-6a9e-4fe9-8b0d-ecd6a0b69388-c000.snappy.parquet\u001b[0m\n",
      "    ├── \u001b[00mpart-00003-ab42755f-6a9e-4fe9-8b0d-ecd6a0b69388-c000.snappy.parquet\u001b[0m\n",
      "    └── \u001b[00mpart-00007-ab42755f-6a9e-4fe9-8b0d-ecd6a0b69388-c000.snappy.parquet\u001b[0m\n",
      "\n",
      "1 directory, 4 files\n"
     ]
    }
   ],
   "source": [
    "!tree tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "081861c1-63d6-4e36-8c4f-414a0a66b9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/30 17:45:38 WARN HadoopFSUtils: The directory file:/Users/powers/Documents/code/my_apps/delta-examples/notebooks/pyspark/spark-warehouse/tmp/unmanaged_data was not found. Was it deleted very recently?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"drop table if exists your_unmanaged_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ad52dc4-0e21-4adf-82de-54c1fe9dfbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d9fe21a9-25e6-43e2-9d4f-6b20d1665a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mtmp/\u001b[0m\n",
      "└── \u001b[01;34munmanaged_data\u001b[0m\n",
      "    ├── \u001b[00m_SUCCESS\u001b[0m\n",
      "    ├── \u001b[00mpart-00000-ab42755f-6a9e-4fe9-8b0d-ecd6a0b69388-c000.snappy.parquet\u001b[0m\n",
      "    ├── \u001b[00mpart-00003-ab42755f-6a9e-4fe9-8b0d-ecd6a0b69388-c000.snappy.parquet\u001b[0m\n",
      "    └── \u001b[00mpart-00007-ab42755f-6a9e-4fe9-8b0d-ecd6a0b69388-c000.snappy.parquet\u001b[0m\n",
      "\n",
      "1 directory, 4 files\n"
     ]
    }
   ],
   "source": [
    "!tree tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad08ba1-8ef8-49b5-8e89-be5e6f4b4bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark-330-delta-210] *",
   "language": "python",
   "name": "conda-env-pyspark-330-delta-210-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
