{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1827b66d-3834-460d-8589-9e20770c35a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from delta import *\n",
    "from pyspark.sql.types import StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b473fb7-ad0a-42f4-8213-7044ee20d69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = (\n",
    "    pyspark.sql.SparkSession.builder.appName(\"MyApp\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62b47844-ab5f-41ab-968d-38e8bbd3e969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/03 14:07:43 WARN Utils: Your hostname, Matthews-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.1.2 instead (on interface en0)\n",
      "22/06/03 14:07:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/powers/.sdkman/candidates/spark/3.2.0/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/powers/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/powers/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-6fdd8347-a3b1-4cef-983d-1e94026cdebd;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;1.2.1 in central\n",
      "\tfound io.delta#delta-storage;1.2.1 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      ":: resolution report :: resolve 317ms :: artifacts dl 23ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;1.2.1 from central in [default]\n",
      "\tio.delta#delta-storage;1.2.1 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-6fdd8347-a3b1-4cef-983d-1e94026cdebd\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/11ms)\n",
      "22/06/03 14:07:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9842d7d0-a20b-413c-855e-31bd7e8702bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = (\n",
    "    StructType()\n",
    "    .add(\"student_name\", \"string\")\n",
    "    .add(\"graduation_year\", \"string\")\n",
    "    .add(\"major\", \"string\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eef9ef85-124c-4f44-82d7-df51a248e7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def with_normalized_names(df):\n",
    "    split_col = pyspark.sql.functions.split(df[\"student_name\"], \"XX\")\n",
    "    return (\n",
    "        df.withColumn(\"first_name\", split_col.getItem(0))\n",
    "        .withColumn(\"last_name\", split_col.getItem(1))\n",
    "        .drop(\"student_name\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483189cc-bb74-48a5-95fc-87c038a3eae7",
   "metadata": {},
   "source": [
    "## Trigger Once Incremental Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "33280822-24ae-487c-a512-de24292ae8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir data/tmp_students_incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8a808b78-b37a-4e91-afed-b65ac576eb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp data/students/students1.csv data/tmp_students_incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9670de34-d120-499b-b15c-15494d06b964",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.readStream.schema(schema)\n",
    "    .option(\"header\", True)\n",
    "    .csv(\"data/tmp_students_incremental\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a4606a22-86c5-43ab-b4a7-8daa48a3ef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_trigger_once_update():\n",
    "    checkpointPath = \"data/tmp_students_checkpoint/\"\n",
    "    deltaPath = \"data/tmp_students_delta\"\n",
    "    return (\n",
    "        df.transform(lambda df: with_normalized_names(df))\n",
    "        .writeStream.trigger(once=True)\n",
    "        .format(\"delta\")\n",
    "        .option(\"checkpointLocation\", checkpointPath)\n",
    "        .start(deltaPath)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e945836a-38a4-4e38-99c1-a625d40a3764",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/03 12:30:45 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x167a6e1c0>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "perform_trigger_once_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "011dbfdb-9911-43a0-aa43-99986301ec92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+----------+---------+\n",
      "|graduation_year|  major|first_name|last_name|\n",
      "+---------------+-------+----------+---------+\n",
      "|           2023|   math|      some|   person|\n",
      "|           2025|physics|        li|      yao|\n",
      "+---------------+-------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(deltaPath).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "042ef719-3601-456a-8d62-425743890495",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp data/students/students2.csv data/tmp_students_incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f70db84a-7eca-4537-8da2-194b38404ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/03 12:31:02 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x167a68ac0>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "perform_trigger_once_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "65fbaffb-043e-4f35-83f3-c33c317d60a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+----------+---------+\n",
      "|graduation_year|  major|first_name|last_name|\n",
      "+---------------+-------+----------+---------+\n",
      "|           2022|    bio|    sophia|     raul|\n",
      "|           2025|physics|      fred|       li|\n",
      "|           2023|   math|      some|   person|\n",
      "|           2025|physics|        li|      yao|\n",
      "+---------------+-------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(deltaPath).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a731925d-07e0-416d-ae6e-340757f174c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp data/students/students3.csv data/tmp_students_incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f090a374-4b13-4de0-ba58-b376704c7d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/03 12:31:13 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x167ab6eb0>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "perform_trigger_once_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c2c86fee-17a6-4fc6-9c9d-7d23ec4c4d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+----------+---------+\n",
      "|graduation_year|  major|first_name|last_name|\n",
      "+---------------+-------+----------+---------+\n",
      "|           2025|    bio|     chris|     borg|\n",
      "|           2026|physics|     david|    cross|\n",
      "|           2022|    bio|    sophia|     raul|\n",
      "|           2025|physics|      fred|       li|\n",
      "|           2023|   math|      some|   person|\n",
      "|           2025|physics|        li|      yao|\n",
      "+---------------+-------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(deltaPath).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e108853-97b1-46e9-adb3-e0ac75e6e975",
   "metadata": {},
   "source": [
    "## Clean up directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0184c4ff-1ac2-47e8-8bb2-d7a9387b6452",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/03 13:48:20 WARN HadoopFSUtils: The directory file:/Users/powers/Documents/code/my_apps/delta-examples/notebooks/data/tmp_students_incremental was not found. Was it deleted very recently?\n",
      "22/06/03 13:48:22 WARN HadoopFSUtils: The directory file:/Users/powers/Documents/code/my_apps/delta-examples/notebooks/data/tmp_students_incremental was not found. Was it deleted very recently?\n",
      "22/06/03 13:48:24 WARN HadoopFSUtils: The directory file:/Users/powers/Documents/code/my_apps/delta-examples/notebooks/data/tmp_students_incremental was not found. Was it deleted very recently?\n",
      "22/06/03 13:48:26 WARN HadoopFSUtils: The directory file:/Users/powers/Documents/code/my_apps/delta-examples/notebooks/data/tmp_students_incremental was not found. Was it deleted very recently?\n",
      "22/06/03 13:48:28 WARN HadoopFSUtils: The directory file:/Users/powers/Documents/code/my_apps/delta-examples/notebooks/data/tmp_students_incremental was not found. Was it deleted very recently?\n"
     ]
    }
   ],
   "source": [
    "! rm -rf data/tmp_students_checkpoint\n",
    "! rm -rf data/tmp_students_delta\n",
    "! rm -rf data/tmp_students_incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c2ab28fa-a6eb-4b0b-b11b-6549b357a2da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "489d97d7-6a4e-45fc-9c13-65780fa0089e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "004803e8-774e-40b3-ad90-0a3c7a3d2920",
   "metadata": {},
   "source": [
    "# ProcessingTime trigger with two-seconds micro-batch interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dde84d6-8577-49c0-9a61-ebab52299791",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir data/tmp_students_incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c26a80f6-129e-40d2-96f5-d930f5cac83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp data/students/students1.csv data/tmp_students_incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15a49d07-fc5d-413d-999f-49f06493ad78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.readStream.schema(schema)\n",
    "    .option(\"header\", True)\n",
    "    .csv(\"data/tmp_students_incremental\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04c510b3-84c5-4524-8f83-b6c0631e564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointPath = \"data/tmp_students_checkpoint/\"\n",
    "deltaPath = \"data/tmp_students_delta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9a4d683-8d79-420c-a17f-b4f709062c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/03 13:53:07 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x163995a60>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                         (0 + 8) / 50]\r"
     ]
    }
   ],
   "source": [
    "df.transform(lambda df: with_normalized_names(df)).writeStream.trigger(\n",
    "    processingTime=\"2 seconds\"\n",
    ").format(\"delta\").option(\"checkpointLocation\", checkpointPath).start(deltaPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7831579-1c08-4cb6-a3d0-9029f4ed4f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/03 13:53:25 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 2000 milliseconds, but spent 16670 milliseconds\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+----------+---------+\n",
      "|graduation_year|  major|first_name|last_name|\n",
      "+---------------+-------+----------+---------+\n",
      "|           2023|   math|      some|   person|\n",
      "|           2025|physics|        li|      yao|\n",
      "+---------------+-------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(deltaPath).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "56ab6a15-43a4-48fd-b470-52c578b5deb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp data/students/students2.csv data/tmp_students_incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a3460a11-5653-4209-b956-dc6f8f010ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+----------+---------+\n",
      "|graduation_year|  major|first_name|last_name|\n",
      "+---------------+-------+----------+---------+\n",
      "|           2023|   math|      some|   person|\n",
      "|           2025|physics|        li|      yao|\n",
      "|           2022|    bio|    sophia|     raul|\n",
      "|           2025|physics|      fred|       li|\n",
      "+---------------+-------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(deltaPath).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "18f92785-c5b1-4d32-a990-90b5bfc1bc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp data/students/students3.csv data/tmp_students_incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "758f8428-53ca-4248-8b31-3fe3cb04dd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+----------+---------+\n",
      "|graduation_year|  major|first_name|last_name|\n",
      "+---------------+-------+----------+---------+\n",
      "|           2025|    bio|     chris|     borg|\n",
      "|           2026|physics|     david|    cross|\n",
      "|           2023|   math|      some|   person|\n",
      "|           2025|physics|        li|      yao|\n",
      "|           2022|    bio|    sophia|     raul|\n",
      "|           2025|physics|      fred|       li|\n",
      "+---------------+-------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(deltaPath).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67a28879-85ef-4d44-9542-108e6a16c3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/03 14:03:28 WARN HadoopFSUtils: The directory file:/Users/powers/Documents/code/my_apps/delta-examples/notebooks/data/tmp_students_incremental was not found. Was it deleted very recently?\n",
      "22/06/03 14:03:30 WARN HadoopFSUtils: The directory file:/Users/powers/Documents/code/my_apps/delta-examples/notebooks/data/tmp_students_incremental was not found. Was it deleted very recently?\n",
      "22/06/03 14:03:32 WARN HadoopFSUtils: The directory file:/Users/powers/Documents/code/my_apps/delta-examples/notebooks/data/tmp_students_incremental was not found. Was it deleted very recently?\n",
      "22/06/03 14:03:34 WARN HadoopFSUtils: The directory file:/Users/powers/Documents/code/my_apps/delta-examples/notebooks/data/tmp_students_incremental was not found. Was it deleted very recently?\n",
      "22/06/03 14:03:36 WARN HadoopFSUtils: The directory file:/Users/powers/Documents/code/my_apps/delta-examples/notebooks/data/tmp_students_incremental was not found. Was it deleted very recently?\n",
      "22/06/03 14:03:38 WARN HadoopFSUtils: The directory file:/Users/powers/Documents/code/my_apps/delta-examples/notebooks/data/tmp_students_incremental was not found. Was it deleted very recently?\n",
      "22/06/03 14:03:40 WARN HadoopFSUtils: The directory file:/Users/powers/Documents/code/my_apps/delta-examples/notebooks/data/tmp_students_incremental was not found. Was it deleted very recently?\n"
     ]
    }
   ],
   "source": [
    "! rm -rf data/tmp_students_checkpoint\n",
    "! rm -rf data/tmp_students_delta\n",
    "! rm -rf data/tmp_students_incremental"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e1ee67-7e71-491d-9918-35230a6d6902",
   "metadata": {},
   "source": [
    "## Read streaming CSV data directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d1ba82c-f8a1-4ace-8b4e-9be535ef7955",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir data/tmp_students_incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e72cf4e-85e4-48a0-9b95-18f9a5192c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp data/students/students1.csv data/tmp_students_incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1bad632-a23b-4b05-b1da-61a15b116375",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.readStream.schema(schema)\n",
    "    .option(\"header\", True)\n",
    "    .csv(\"data/tmp_students_incremental\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a6f25e1-ad93-48f1-8ab4-d2f898694548",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointPath = \"data/tmp_students_checkpoint/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c709a2e0-4013-4353-8e3e-c2f1c3a9c7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/03 14:15:28 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "22/06/03 14:15:28 WARN StreamingQueryManager: Stopping existing streaming query [id=3cb90a75-80d0-4d39-ad88-ca1e6bd7c525, runId=7fd74226-4536-4b34-a44d-a3a7c1d70326], as a new run is being started.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x164e621c0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.transform(lambda df: with_normalized_names(df)).writeStream.format(\n",
    "    \"console\"\n",
    ").trigger(processingTime='1 seconds').option(\"checkpointLocation\", checkpointPath).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb294a30-9217-4d4e-b624-ba48b1432e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+---------------+-------+----------+---------+\n",
      "|graduation_year|  major|first_name|last_name|\n",
      "+---------------+-------+----------+---------+\n",
      "|           2022|    bio|    sophia|     raul|\n",
      "|           2025|physics|      fred|       li|\n",
      "+---------------+-------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! cp data/students/students2.csv data/tmp_students_incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3234bd7e-8b59-4f0a-b00a-e1b260b05259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+---------------+-------+----------+---------+\n",
      "|graduation_year|  major|first_name|last_name|\n",
      "+---------------+-------+----------+---------+\n",
      "|           2025|    bio|     chris|     borg|\n",
      "|           2026|physics|     david|    cross|\n",
      "+---------------+-------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! cp data/students/students3.csv data/tmp_students_incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c16f45-7937-4413-ae8d-0008a1ddf2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mr-delta] *",
   "language": "python",
   "name": "conda-env-mr-delta-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
