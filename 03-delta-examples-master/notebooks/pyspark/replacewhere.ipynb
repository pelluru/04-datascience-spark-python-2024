{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "The Delta Lake [`replaceWhere`](https://mungingdata.com/delta-lake/updating-partitions-with-replacewhere/) option allows users to selectively apply updates to specific data partitions rather than to full lakes, which may result in significant speed gains. This notebook briefly illustrates the usage of `replaceWhere` option. For more details, see:\n",
    "- [Selectively updating Delta partitions with replaceWhere](https://mungingdata.com/delta-lake/updating-partitions-with-replacewhere/) (this notebook will be following the example from this blog)\n",
    "- [Selectively overwrite data with Delta Lake](https://docs.databricks.com/delta/selective-overwrite.html)\n",
    "- [Table batch reads and writes: overwrite](https://docs.delta.io/latest/delta-batch.html#overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/14 14:32:49 WARN Utils: Your hostname, Richards-MacBook-Pro-2.local resolves to a loopback address: 127.0.0.1; using 172.20.10.4 instead (on interface en0)\n",
      "23/12/14 14:32:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/rpelgrim/miniforge3/envs/pyspark-340-delta-240/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/rpelgrim/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/rpelgrim/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-032475ef-69bc-4905-9c8c-c5eec64e57ec;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.4.0 in central\n",
      "\tfound io.delta#delta-storage;2.4.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 266ms :: artifacts dl 14ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.4.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.4.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-032475ef-69bc-4905-9c8c-c5eec64e57ec\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/9ms)\n",
      "23/12/14 14:32:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from delta import *\n",
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "builder = (\n",
    "    pyspark.sql.SparkSession.builder.appName(\"MyApp\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    ")\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple replaceWhere example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(\"a\", 1), (\"b\", 2), (\"c\", 3), (\"d\", 4)]).toDF(\n",
    "    \"letter\", \"number\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/14 14:33:11 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.format(\"delta\").save(\"tmp/my_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|letter|number|\n",
      "+------+------+\n",
      "|     a|     1|\n",
      "|     b|     2|\n",
      "|     c|     3|\n",
      "|     d|     4|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(\"tmp/my_data\").orderBy(col(\"number\").asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.createDataFrame(\n",
    "    [\n",
    "        (\"x\", 7),\n",
    "        (\"y\", 8),\n",
    "        (\"z\", 9),\n",
    "    ]\n",
    ").toDF(\"letter\", \"number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|letter|number|\n",
      "+------+------+\n",
      "|     x|     7|\n",
      "|     y|     8|\n",
      "|     z|     9|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df2.write.format(\"delta\")\n",
    "    .option(\"replaceWhere\", \"number > 2\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"tmp/my_data\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|letter|number|\n",
      "+------+------+\n",
      "|     a|     1|\n",
      "|     b|     2|\n",
      "|     x|     7|\n",
      "|     y|     8|\n",
      "|     z|     9|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(\"tmp/my_data\").orderBy(col(\"number\").asc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Simple replaceWhere example with partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (\"aa\", 11),\n",
    "        (\"bb\", 22),\n",
    "        (\"aa\", 33),\n",
    "        (\"cc\", 33),\n",
    "    ]\n",
    ").toDF(\"patient_id\", \"medical_code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").partitionBy(\"medical_code\").save(\"tmp/patients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mtmp/patients\u001b[0m\n",
      "├── \u001b[01;34m_delta_log\u001b[0m\n",
      "│   └── \u001b[00m00000000000000000000.json\u001b[0m\n",
      "├── \u001b[01;34mmedical_code=11\u001b[0m\n",
      "│   └── \u001b[00mpart-00002-49a164ed-7590-4d4c-8216-bc1a6947ff3b.c000.snappy.parquet\u001b[0m\n",
      "├── \u001b[01;34mmedical_code=22\u001b[0m\n",
      "│   └── \u001b[00mpart-00004-8364a37a-f5d8-4cfa-8daa-065b5760bedd.c000.snappy.parquet\u001b[0m\n",
      "└── \u001b[01;34mmedical_code=33\u001b[0m\n",
      "    ├── \u001b[00mpart-00007-522512ed-d6ad-4c3f-996d-5a737b12030b.c000.snappy.parquet\u001b[0m\n",
      "    └── \u001b[00mpart-00009-d708e56b-0d87-4545-b3b7-9fc4d3053560.c000.snappy.parquet\u001b[0m\n",
      "\n",
      "4 directories, 5 files\n"
     ]
    }
   ],
   "source": [
    "!tree tmp/patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|patient_id|medical_code|\n",
      "+----------+------------+\n",
      "|        aa|          11|\n",
      "|        bb|          22|\n",
      "|        aa|          33|\n",
      "|        cc|          33|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    spark.read.format(\"delta\")\n",
    "    .load(\"tmp/patients\")\n",
    "    .orderBy(col(\"medical_code\").asc())\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.createDataFrame(\n",
    "    [\n",
    "        (\"dd\", 33),\n",
    "        (\"f\", 33),\n",
    "    ]\n",
    ").toDF(\"patient_id\", \"medical_code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df2.write.format(\"delta\")\n",
    "    .option(\"replaceWhere\", \"medical_code = '33'\")\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"medical_code\")\n",
    "    .save(\"tmp/patients\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|patient_id|medical_code|\n",
      "+----------+------------+\n",
      "|        aa|          11|\n",
      "|        bb|          22|\n",
      "|        dd|          33|\n",
      "|         f|          33|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    spark.read.format(\"delta\")\n",
    "    .load(\"tmp/patients\")\n",
    "    .orderBy(col(\"medical_code\").asc())\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More complicated Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load some Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+---------+\n",
      "|first_name|last_name|  country|continent|\n",
      "+----------+---------+---------+---------+\n",
      "|   Ernesto|  Guevara|Argentina|     null|\n",
      "|     Bruce|      Lee|    China|     null|\n",
      "|      Jack|       Ma|    China|     null|\n",
      "|  Wolfgang|   Manche|  Germany|     null|\n",
      "|    Soraya|     Jala|  Germany|     null|\n",
      "+----------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.options(header=\"True\", charset=\"UTF8\").csv(\n",
    "    \"../../data/people_countries.csv\"\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition on Country\n",
    "Now we'll repartition the DataFrame on `country` and write it to disk in the Delta Lake format, partitioned by `country`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "deltaPath = \"../../data/people_countries_delta/\"\n",
    "\n",
    "(\n",
    "    df.repartition(col(\"country\"))\n",
    "    .write.partitionBy(\"country\")\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(deltaPath)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write a function to add `continent` values to a DataFrame based on the value of `country`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "\n",
    "def withContinent(df):\n",
    "    return df.withColumn(\n",
    "        \"continent\",\n",
    "        when(col(\"country\") == \"Germany\", \"Europe\")\n",
    "        .when(col(\"country\") == \"China\", \"Asia\")\n",
    "        .when(col(\"country\") == \"Argentina\", \"South America\"),\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's where `replaceWhere` comes in. Suppose we only want to populate the `continent` column when `country == 'China'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(deltaPath)\n",
    "df = df.where(col(\"country\") == \"China\").transform(withContinent)\n",
    "\n",
    "(\n",
    "    df.write.format(\"delta\")\n",
    "    .option(\"replaceWhere\", \"country = 'China'\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(deltaPath)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+---------+\n",
      "|first_name|last_name|country  |continent|\n",
      "+----------+---------+---------+---------+\n",
      "|Bruce     |Lee      |China    |Asia     |\n",
      "|Jack      |Ma       |China    |Asia     |\n",
      "|Ernesto   |Guevara  |Argentina|null     |\n",
      "|Wolfgang  |Manche   |Germany  |null     |\n",
      "|Soraya    |Jala     |Germany  |null     |\n",
      "+----------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(deltaPath).show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happened by taking a look at the most recent log:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"add\": {\n",
      "        \"path\": \"country=China/part-00000-adf67d14-a5a1-4f0f-8d7c-99cb5bb8b2dd.c000.snappy.parquet\",\n",
      "        \"partitionValues\": {\n",
      "            \"country\": \"China\"\n",
      "        },\n",
      "        \"size\": 1002,\n",
      "        \"modificationTime\": 1702564443485,\n",
      "        \"dataChange\": true,\n",
      "        \"stats\": \"{\\\"numRecords\\\":2,\\\"minValues\\\":{\\\"first_name\\\":\\\"Bruce\\\",\\\"last_name\\\":\\\"Lee\\\",\\\"continent\\\":\\\"Asia\\\"},\\\"maxValues\\\":{\\\"first_name\\\":\\\"Jack\\\",\\\"last_name\\\":\\\"Ma\\\",\\\"continent\\\":\\\"Asia\\\"},\\\"nullCount\\\":{\\\"first_name\\\":0,\\\"last_name\\\":0,\\\"continent\\\":0}}\"\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"remove\": {\n",
      "        \"path\": \"country=China/part-00000-2fae942f-c7e3-450a-aa4f-4fe991d84c5f.c000.snappy.parquet\",\n",
      "        \"deletionTimestamp\": 1702564441288,\n",
      "        \"dataChange\": true,\n",
      "        \"extendedFileMetadata\": true,\n",
      "        \"partitionValues\": {\n",
      "            \"country\": \"China\"\n",
      "        },\n",
      "        \"size\": 929\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "\n",
    "# get path to latest log\n",
    "path_to_logs = str(deltaPath + \"_delta_log/*.json\")\n",
    "list_of_logs = glob.glob(path_to_logs)\n",
    "latest_log = max(list_of_logs, key=os.path.getctime)\n",
    "latest_log\n",
    "\n",
    "# open latest log\n",
    "with open(latest_log, \"r\") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        if \"add\" in data or \"remove\" in data:\n",
    "            print(json.dumps(data, indent=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that only the `country=China/part-00000-87aebbc2-aff3-4bd6-b369-aa9aacbb93be.c000.snappy.parquet` file was modified. The other partitions were not.\n",
    "\n",
    "For more details, read the [blog post]()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Multiple Partitions\n",
    "Let's go one step further to see how we can use `replaceWhere` to update rows spread over multiple partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by creating a Delta table with multiple countries in the same continent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+---------+\n",
      "|first_name|last_name|  country|continent|\n",
      "+----------+---------+---------+---------+\n",
      "|   Ernesto|  Guevara|Argentina|     null|\n",
      "|     Bruce|      Lee|    China|     null|\n",
      "|      Jack|       Ma|    China|     null|\n",
      "|  Wolfgang|   Manche|  Germany|     null|\n",
      "|    Soraya|     Jala|  Germany|     null|\n",
      "+----------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (\n",
    "    spark.read.options(header=\"True\", charset=\"UTF8\")\n",
    "    .csv(\"../../data/people_countries.csv\")\n",
    "    .withColumn(\"continent\", lit(None).cast(StringType()))\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+-------------+\n",
      "|first_name|last_name|  country|    continent|\n",
      "+----------+---------+---------+-------------+\n",
      "|   Ernesto|  Guevara|Argentina|South America|\n",
      "|     Bruce|      Lee|    China|         Asia|\n",
      "|      Jack|       Ma|    China|         Asia|\n",
      "|  Wolfgang|   Manche|  Germany|       Europe|\n",
      "|    Soraya|     Jala|  Germany|       Europe|\n",
      "+----------+---------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add continents to all\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "\n",
    "def withContinent(df):\n",
    "    return df.withColumn(\n",
    "        \"continent\",\n",
    "        when(col(\"country\") == \"Germany\", \"Europe\")\n",
    "        .when(col(\"country\") == \"China\", \"Asia\")\n",
    "        .when(col(\"country\") == \"Argentina\", \"South America\"),\n",
    "    )\n",
    "\n",
    "\n",
    "df = df.transform(withContinent)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "deltaPath = \"../../data/people_countries_delta/\"\n",
    "\n",
    "(\n",
    "    df.repartition(col(\"country\"))\n",
    "    .write.partitionBy(\"country\")\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(deltaPath)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+-------------+\n",
      "|first_name|last_name|  country|    continent|\n",
      "+----------+---------+---------+-------------+\n",
      "|   Ernesto|  Guevara|Argentina|South America|\n",
      "|  Wolfgang|   Manche|  Germany|       Europe|\n",
      "|    Soraya|     Jala|  Germany|       Europe|\n",
      "|     Bruce|      Lee|    China|         Asia|\n",
      "|      Jack|       Ma|    China|         Asia|\n",
      "+----------+---------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read to confirm\n",
    "spark.read.format(\"delta\").load(deltaPath).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a second DataFrame with 3 more entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+---------+\n",
      "|first_name|last_name| country|continent|\n",
      "+----------+---------+--------+---------+\n",
      "|     Hamed|   Snouba| Lebanon|     Asia|\n",
      "|   Jasmine| Terrywin|Thailand|     Asia|\n",
      "|   Janneke|    Bosma| Belgium|   Europe|\n",
      "+----------+---------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# append df with more countries\n",
    "df2 = spark.createDataFrame(\n",
    "    [\n",
    "        (\"Hamed\", \"Snouba\", \"Lebanon\", \"Asia\"),\n",
    "        (\"Jasmine\", \"Terrywin\", \"Thailand\", \"Asia\"),\n",
    "        (\"Janneke\", \"Bosma\", \"Belgium\", \"Europe\"),\n",
    "    ]\n",
    ").toDF(\"first_name\", \"last_name\", \"country\", \"continent\")\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# append new rows\n",
    "(df2.write.format(\"delta\").mode(\"append\").save(deltaPath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+-------------+\n",
      "|first_name|last_name|  country|    continent|\n",
      "+----------+---------+---------+-------------+\n",
      "|   Ernesto|  Guevara|Argentina|South America|\n",
      "|  Wolfgang|   Manche|  Germany|       Europe|\n",
      "|    Soraya|     Jala|  Germany|       Europe|\n",
      "|   Jasmine| Terrywin| Thailand|         Asia|\n",
      "|   Janneke|    Bosma|  Belgium|       Europe|\n",
      "|     Hamed|   Snouba|  Lebanon|         Asia|\n",
      "|     Bruce|      Lee|    China|         Asia|\n",
      "|      Jack|       Ma|    China|         Asia|\n",
      "+----------+---------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read to confirm\n",
    "df = spark.read.format(\"delta\").load(deltaPath)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m_delta_log\u001b[m\u001b[m        \u001b[34mcountry=Belgium\u001b[m\u001b[m   \u001b[34mcountry=Germany\u001b[m\u001b[m   \u001b[34mcountry=Thailand\u001b[m\u001b[m\n",
      "\u001b[34mcountry=Argentina\u001b[m\u001b[m \u001b[34mcountry=China\u001b[m\u001b[m     \u001b[34mcountry=Lebanon\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# do we still have the correct partitions?\n",
    "! ls ../../data/people_countries_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function\n",
    "from pyspark.sql.functions import translate\n",
    "\n",
    "\n",
    "def anonymizeLastname(df):\n",
    "    return df.withColumn(\"last_name\", translate(\"last_name\", \"aeiou\", \"12345\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+---------+\n",
      "|first_name|last_name| country|continent|\n",
      "+----------+---------+--------+---------+\n",
      "|   Jasmine| T2rryw3n|Thailand|     Asia|\n",
      "|     Hamed|   Sn45b1| Lebanon|     Asia|\n",
      "|     Bruce|      L22|   China|     Asia|\n",
      "|      Jack|       M1|   China|     Asia|\n",
      "+----------+---------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# perform a replaceWhere on a continent == \"Asia\"\n",
    "df = df.where(col(\"continent\") == \"Asia\").transform(anonymizeLastname)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# (selective) overwrite to disk\n",
    "(\n",
    "    df.write.format(\"delta\")\n",
    "    .option(\"replaceWhere\", \"continent = 'Asia'\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(deltaPath)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+-------------+\n",
      "|first_name|last_name|  country|    continent|\n",
      "+----------+---------+---------+-------------+\n",
      "|   Ernesto|  Guevara|Argentina|South America|\n",
      "|  Wolfgang|   Manche|  Germany|       Europe|\n",
      "|    Soraya|     Jala|  Germany|       Europe|\n",
      "|   Jasmine| T2rryw3n| Thailand|         Asia|\n",
      "|   Janneke|    Bosma|  Belgium|       Europe|\n",
      "|     Hamed|   Sn45b1|  Lebanon|         Asia|\n",
      "|     Bruce|      L22|    China|         Asia|\n",
      "|      Jack|       M1|    China|         Asia|\n",
      "+----------+---------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(deltaPath)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job! \n",
    "\n",
    "Let's just check the most recent log to confirm what happened:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"add\": {\n",
      "        \"path\": \"country=Thailand/part-00000-90e36b14-623b-455b-917a-11a6063ecccb.c000.snappy.parquet\",\n",
      "        \"partitionValues\": {\n",
      "            \"country\": \"Thailand\"\n",
      "        },\n",
      "        \"size\": 1032,\n",
      "        \"modificationTime\": 1702406183349,\n",
      "        \"dataChange\": true,\n",
      "        \"stats\": \"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"first_name\\\":\\\"Jasmine\\\",\\\"last_name\\\":\\\"T2rryw3n\\\",\\\"continent\\\":\\\"Asia\\\"},\\\"maxValues\\\":{\\\"first_name\\\":\\\"Jasmine\\\",\\\"last_name\\\":\\\"T2rryw3n\\\",\\\"continent\\\":\\\"Asia\\\"},\\\"nullCount\\\":{\\\"first_name\\\":0,\\\"last_name\\\":0,\\\"continent\\\":0}}\"\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"add\": {\n",
      "        \"path\": \"country=Lebanon/part-00001-e419556d-7d8d-4263-b6fd-915a4edff62b.c000.snappy.parquet\",\n",
      "        \"partitionValues\": {\n",
      "            \"country\": \"Lebanon\"\n",
      "        },\n",
      "        \"size\": 1004,\n",
      "        \"modificationTime\": 1702406183349,\n",
      "        \"dataChange\": true,\n",
      "        \"stats\": \"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"first_name\\\":\\\"Hamed\\\",\\\"last_name\\\":\\\"Sn45b1\\\",\\\"continent\\\":\\\"Asia\\\"},\\\"maxValues\\\":{\\\"first_name\\\":\\\"Hamed\\\",\\\"last_name\\\":\\\"Sn45b1\\\",\\\"continent\\\":\\\"Asia\\\"},\\\"nullCount\\\":{\\\"first_name\\\":0,\\\"last_name\\\":0,\\\"continent\\\":0}}\"\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"add\": {\n",
      "        \"path\": \"country=China/part-00002-0a628002-85f9-450c-9340-4b01ef225e0e.c000.snappy.parquet\",\n",
      "        \"partitionValues\": {\n",
      "            \"country\": \"China\"\n",
      "        },\n",
      "        \"size\": 1002,\n",
      "        \"modificationTime\": 1702406183354,\n",
      "        \"dataChange\": true,\n",
      "        \"stats\": \"{\\\"numRecords\\\":2,\\\"minValues\\\":{\\\"first_name\\\":\\\"Bruce\\\",\\\"last_name\\\":\\\"L22\\\",\\\"continent\\\":\\\"Asia\\\"},\\\"maxValues\\\":{\\\"first_name\\\":\\\"Jack\\\",\\\"last_name\\\":\\\"M1\\\",\\\"continent\\\":\\\"Asia\\\"},\\\"nullCount\\\":{\\\"first_name\\\":0,\\\"last_name\\\":0,\\\"continent\\\":0}}\"\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"remove\": {\n",
      "        \"path\": \"country=Thailand/part-00005-1f073a57-dca5-4690-9f9c-ffebb5912b75.c000.snappy.parquet\",\n",
      "        \"deletionTimestamp\": 1702406182360,\n",
      "        \"dataChange\": true,\n",
      "        \"extendedFileMetadata\": true,\n",
      "        \"partitionValues\": {\n",
      "            \"country\": \"Thailand\"\n",
      "        },\n",
      "        \"size\": 1032\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"remove\": {\n",
      "        \"path\": \"country=Lebanon/part-00002-7594dbc1-85d4-4f08-980f-4a2e56420b3a.c000.snappy.parquet\",\n",
      "        \"deletionTimestamp\": 1702406182360,\n",
      "        \"dataChange\": true,\n",
      "        \"extendedFileMetadata\": true,\n",
      "        \"partitionValues\": {\n",
      "            \"country\": \"Lebanon\"\n",
      "        },\n",
      "        \"size\": 1004\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"remove\": {\n",
      "        \"path\": \"country=China/part-00000-e364c08a-1db9-4736-9fa6-c51b7b72caa2.c000.snappy.parquet\",\n",
      "        \"deletionTimestamp\": 1702406182360,\n",
      "        \"dataChange\": true,\n",
      "        \"extendedFileMetadata\": true,\n",
      "        \"partitionValues\": {\n",
      "            \"country\": \"China\"\n",
      "        },\n",
      "        \"size\": 1002\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# get path to latest log\n",
    "path_to_logs = str(deltaPath + \"_delta_log/*.json\")\n",
    "list_of_logs = glob.glob(path_to_logs)\n",
    "latest_log = max(list_of_logs, key=os.path.getctime)\n",
    "latest_log\n",
    "\n",
    "# open latest log\n",
    "with open(latest_log, \"r\") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        if \"add\" in data or \"remove\" in data:\n",
    "            print(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice work - only the partitions for the countries in Asia were affected by our `replaceWhere` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the full blog\n",
    "This was just a quick demonstration. For the full walkthrough with detailed explanation, check out [the blog]()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df.write.format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"partitionOverwriteMode\", \"dynamic\")\n",
    "    .saveAsTable(\"default.people10m\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-340-delta-240",
   "language": "python",
   "name": "pyspark-340-delta-240"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
