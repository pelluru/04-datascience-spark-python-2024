{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delta Lake fundamentals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was built to be run on the following docker image: `jupyter/pyspark-notebook:spark-3.3.1`\n",
    "\n",
    "https://towardsdatascience.com/hands-on-introduction-to-delta-lake-with-py-spark-b39460a4b1ae\n",
    "\n",
    "https://github.com/jaumpedro214/posts/tree/main\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyspark\n",
    "# !pip install delta-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DoubleType\n",
    "# import pyspark.sql.functions as F\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DoubleType\n",
    "from delta import *\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "builder = (\n",
    "    pyspark.sql.SparkSession.builder.appName(\"MyApp\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta Lake fundamentals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create a Delta Table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMA = StructType(\n",
    "    [\n",
    "        StructField('id', StringType(), True), \n",
    "        StructField('data_inversa', StringType(), True), \n",
    "        StructField('dia_semana', StringType(), True), \n",
    "        StructField('horario', StringType(), True), \n",
    "        StructField('uf', StringType(), True), \n",
    "        StructField('br', StringType(), True), \n",
    "        StructField('km', StringType(), True), \n",
    "        StructField('municipio', StringType(), True), \n",
    "        StructField('causa_acidente', StringType(), True), \n",
    "        StructField('tipo_acidente', StringType(), True), \n",
    "        StructField('classificacao_acidente', StringType(), True), \n",
    "        StructField('fase_dia', StringType(), True), \n",
    "        StructField('sentido_via', StringType(), True), \n",
    "        StructField('condicao_metereologica', StringType(), True), \n",
    "        StructField('tipo_pista', StringType(), True), \n",
    "        StructField('tracado_via', StringType(), True), \n",
    "        StructField('uso_solo', StringType(), True), \n",
    "        StructField('pessoas', IntegerType(), True), \n",
    "        StructField('mortos', IntegerType(), True), \n",
    "        StructField('feridos_leves', IntegerType(), True), \n",
    "        StructField('feridos_graves', IntegerType(), True), \n",
    "        StructField('ilesos', IntegerType(), True), \n",
    "        StructField('ignorados', IntegerType(), True), \n",
    "        StructField('feridos', IntegerType(), True), \n",
    "        StructField('veiculos', StringType(), True), \n",
    "        StructField('latitude', DoubleType(), True), \n",
    "        StructField('longitude', DoubleType(), True), \n",
    "        StructField('regional', StringType(), True), \n",
    "        StructField('delegacia', StringType(), True), \n",
    "        StructField('uop', StringType(), True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+------------+--------+---+---+-----+--------------------+--------------------+--------------------+----------------------+---------+-----------+----------------------+----------+-----------+--------+-------+------+-------------+--------------+------+---------+-------+--------+--------+---------+--------+---------+--------------+\n",
      "|    id|data_inversa|  dia_semana| horario| uf| br|   km|           municipio|      causa_acidente|       tipo_acidente|classificacao_acidente| fase_dia|sentido_via|condicao_metereologica|tipo_pista|tracado_via|uso_solo|pessoas|mortos|feridos_leves|feridos_graves|ilesos|ignorados|feridos|veiculos|latitude|longitude|regional|delegacia|           uop|\n",
      "+------+------------+------------+--------+---+---+-----+--------------------+--------------------+--------------------+----------------------+---------+-----------+----------------------+----------+-----------+--------+-------+------+-------------+--------------+------+---------+-------+--------+--------+---------+--------+---------+--------------+\n",
      "|260068|  2020-01-01|quarta-feira|05:40:00| PA|316|   84|SAO FRANCISCO DO ...|Falta de Atenção ...|Saída de leito ca...|                    NA|Pleno dia|Decrescente|             Céu Claro|   Simples|       Reta|     Não|      3|     0|            2|             0|     0|        1|      2|       2|    NULL|     NULL| SPRF-PA| DEL01-PA|UOP02-DEL01-PA|\n",
      "|260073|  2020-01-01|quarta-feira|06:00:00| MG|262|  804|             UBERABA|Falta de Atenção ...| Colisão transversal|   Com Vítimas Feridas|Pleno dia|Decrescente|             Céu Claro|     Dupla|       Reta|     Sim|      4|     0|            1|             0|     3|        0|      1|       2|    NULL|     NULL| SPRF-MG| DEL13-MG|UOP01-DEL13-MG|\n",
      "|260087|  2020-01-01|quarta-feira|06:00:00| BA|116|  191|             CANUDOS|   Condutor Dormindo|Saída de leito ca...|    Com Vítimas Fatais|Pleno dia|  Crescente|               Nublado|   Simples|       Reta|     Não|      2|     1|            0|             0|     0|        2|      0|       3|    NULL|     NULL| SPRF-BA| DEL07-BA|UOP02-DEL07-BA|\n",
      "|260116|  2020-01-01|quarta-feira|10:08:00| SP|116|   71|           APARECIDA|Não guardar distâ...|    Colisão traseira|   Com Vítimas Feridas|Pleno dia|  Crescente|                   Sol|     Dupla|       Reta|     Sim|      3|     0|            2|             0|     1|        0|      2|       2|    NULL|     NULL| SPRF-SP| DEL08-SP|UOP01-DEL08-SP|\n",
      "|260129|  2020-01-01|quarta-feira|12:10:00| MG|262|380,9|             JUATUBA|   Condutor Dormindo|Saída de leito ca...|   Com Vítimas Feridas|Pleno dia|  Crescente|             Céu Claro|     Dupla|      Curva|     Não|      2|     0|            1|             0|     0|        1|      1|       2|    NULL|     NULL| SPRF-MG| DEL01-MG|UOP03-DEL01-MG|\n",
      "+------+------------+------------+--------+---+---+-----+--------------------+--------------------+--------------------+----------------------+---------+-----------+----------------------+----------+-----------+--------+-------+------+-------------+--------------+------+---------+-------+--------+--------+---------+--------+---------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_acidentes = (\n",
    "    spark\n",
    "    .read.format(\"csv\")\n",
    "    .option(\"delimiter\", \";\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"encoding\", \"ISO-8859-1\")\n",
    "    .schema(SCHEMA)\n",
    "    .load(\"data/acidentes/datatran2020.csv\")\n",
    ")\n",
    "\n",
    "df_acidentes.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Delta Table is simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acidentes\\\n",
    "    .write.format(\"delta\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save(\"data_lake/delta/acidentes/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: If you are having trouble in writing the Delta Table, remember to check the permissions of the folder, including the newly created._"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read from a Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acidentes_delta = (\n",
    "    spark\n",
    "    .read.format(\"delta\")\n",
    "    .load(\"data_lake/delta/acidentes/\")\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can execute queries as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+------------+--------+---+\n",
      "|    id|data_inversa|  dia_semana| horario| uf|\n",
      "+------+------------+------------+--------+---+\n",
      "|260068|  2020-01-01|quarta-feira|05:40:00| PA|\n",
      "|260073|  2020-01-01|quarta-feira|06:00:00| MG|\n",
      "|260087|  2020-01-01|quarta-feira|06:00:00| BA|\n",
      "|260116|  2020-01-01|quarta-feira|10:08:00| SP|\n",
      "|260129|  2020-01-01|quarta-feira|12:10:00| MG|\n",
      "+------+------------+------------+--------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_acidentes_delta.select([\"id\", \"data_inversa\", \"dia_semana\", \"horario\", \"uf\"]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63585"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acidentes_delta.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Add new data to the Delta Table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding new data is just a matter of appending the new data to the Delta Table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acidentes_2019 = (\n",
    "    spark\n",
    "    .read.format(\"csv\")\n",
    "    .option(\"delimiter\", \";\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(SCHEMA)\n",
    "    .load(\"data/acidentes/datatran2019.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acidentes_2019\\\n",
    "    .write.format(\"delta\")\\\n",
    "    .mode(\"append\")\\\n",
    "    .save(\"data_lake/delta/acidentes/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the number of rows in the Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131143"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acidentes_delta.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. View the history (logs) of the Delta Table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Log of the Delta Table is a record of all the operations that have been performed on the table. It contains a detailed description of each operation performed, including all the metadata about the operation.\n",
    "\n",
    "To read the log, we can use a special python object called `DeltaTable`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|     10|2024-12-24 18:43:...|  NULL|    NULL|    WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          9|  Serializable|         true|{numFiles -> 5, n...|        NULL|Apache-Spark/3.5....|\n",
      "|      9|2024-12-24 18:42:...|  NULL|    NULL|    WRITE|{mode -> Overwrit...|NULL|    NULL|     NULL|          8|  Serializable|        false|{numFiles -> 5, n...|        NULL|Apache-Spark/3.5....|\n",
      "|      8|2024-12-24 18:07:...|  NULL|    NULL|    MERGE|{predicate -> [\"(...|NULL|    NULL|     NULL|          7|  Serializable|        false|{numTargetRowsCop...|        NULL|Apache-Spark/3.5....|\n",
      "|      7|2024-12-24 18:06:...|  NULL|    NULL|    WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          6|  Serializable|         true|{numFiles -> 5, n...|        NULL|Apache-Spark/3.5....|\n",
      "|      6|2024-12-24 17:59:...|  NULL|    NULL|   UPDATE|{predicate -> [\"E...|NULL|    NULL|     NULL|          5|  Serializable|        false|{numRemovedFiles ...|        NULL|Apache-Spark/3.5....|\n",
      "|      5|2024-12-24 17:58:...|  NULL|    NULL|    WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          4|  Serializable|         true|{numFiles -> 5, n...|        NULL|Apache-Spark/3.5....|\n",
      "|      4|2024-12-24 17:55:...|  NULL|    NULL|  RESTORE|{version -> 1, ti...|NULL|    NULL|     NULL|          3|  Serializable|        false|{numRestoredFiles...|        NULL|Apache-Spark/3.5....|\n",
      "|      3|2024-12-24 17:55:...|  NULL|    NULL|  RESTORE|{version -> 0, ti...|NULL|    NULL|     NULL|          2|  Serializable|        false|{numRestoredFiles...|        NULL|Apache-Spark/3.5....|\n",
      "|      2|2024-12-24 17:41:...|  NULL|    NULL|    WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          1|  Serializable|         true|{numFiles -> 5, n...|        NULL|Apache-Spark/3.5....|\n",
      "|      1|2024-12-24 17:37:...|  NULL|    NULL|    WRITE|{mode -> Overwrit...|NULL|    NULL|     NULL|          0|  Serializable|        false|{numFiles -> 5, n...|        NULL|Apache-Spark/3.5....|\n",
      "|      0|2024-12-24 16:49:...|  NULL|    NULL|    WRITE|{mode -> Overwrit...|NULL|    NULL|     NULL|       NULL|  Serializable|        false|{numFiles -> 5, n...|        NULL|Apache-Spark/3.5....|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table = DeltaTable.forPath(spark, \"data_lake/delta/acidentes/\")\n",
    "\n",
    "delta_table.history().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+---------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|version|timestamp              |operation|operationParameters                                                                                                                                                                                                          |\n",
      "+-------+-----------------------+---------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|10     |2024-12-24 18:43:10.595|WRITE    |{mode -> Append, partitionBy -> []}                                                                                                                                                                                          |\n",
      "|9      |2024-12-24 18:42:54.971|WRITE    |{mode -> Overwrite, partitionBy -> []}                                                                                                                                                                                       |\n",
      "|8      |2024-12-24 18:07:40.88 |MERGE    |{predicate -> [\"((id#6580 = id#16881) AND (data_inversa#6581 = data_inversa#16882))\"], matchedPredicates -> [{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"actionType\":\"insert\"}], notMatchedBySourcePredicates -> []}|\n",
      "|7      |2024-12-24 18:06:30.044|WRITE    |{mode -> Append, partitionBy -> []}                                                                                                                                                                                          |\n",
      "|6      |2024-12-24 17:59:37.908|UPDATE   |{predicate -> [\"EndsWith(data_inversa#6581, /16)\"]}                                                                                                                                                                          |\n",
      "|5      |2024-12-24 17:58:26.316|WRITE    |{mode -> Append, partitionBy -> []}                                                                                                                                                                                          |\n",
      "|4      |2024-12-24 17:55:39.334|RESTORE  |{version -> 1, timestamp -> NULL}                                                                                                                                                                                            |\n",
      "|3      |2024-12-24 17:55:14.48 |RESTORE  |{version -> 0, timestamp -> NULL}                                                                                                                                                                                            |\n",
      "|2      |2024-12-24 17:41:11.685|WRITE    |{mode -> Append, partitionBy -> []}                                                                                                                                                                                          |\n",
      "|1      |2024-12-24 17:37:26.439|WRITE    |{mode -> Overwrite, partitionBy -> []}                                                                                                                                                                                       |\n",
      "+-------+-----------------------+---------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta_table.history().select(\"version\", \"timestamp\", \"operation\", \"operationParameters\").show(10, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Read a specific version of the Delta Table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If nothing is specified, the Spark will read the latest version of the Delta Table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131143"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acidentes_latest = (\n",
    "    spark\n",
    "    .read.format(\"delta\")\n",
    "    .load(\"data_lake/delta/acidentes/\")\n",
    ")\n",
    "\n",
    "df_acidentes_latest.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it's also possible to read a specific version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63585"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acidentes_version_0 = (\n",
    "    spark\n",
    "    .read.format(\"delta\")\n",
    "    .option(\"versionAsOf\", 0)\n",
    "    .load(\"data_lake/delta/acidentes/\")\n",
    ")\n",
    "\n",
    "df_acidentes_version_0.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the version 0 was specified, it only contains the data from the first operation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Revert to a previous version"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is another operation performed by the `DeltaTable` object. [Link](https://delta.io/blog/2022-10-03-rollback-delta-lake-restore/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/24 18:43:15 WARN DAGScheduler: Broadcasting large task binary with size 1079.8 KiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[table_size_after_restore: bigint, num_of_files_after_restore: bigint, num_removed_files: bigint, num_restored_files: bigint, removed_files_size: bigint, restored_files_size: bigint]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_table.restoreToVersion(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the latest version doesn't contain the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63585"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acidentes_latest.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **RESTORE** operation is also stored in the log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+---------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|version|timestamp              |operation|operationParameters                                                                                                                                                                                                          |\n",
      "+-------+-----------------------+---------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|11     |2024-12-24 18:43:17.158|RESTORE  |{version -> 0, timestamp -> NULL}                                                                                                                                                                                            |\n",
      "|10     |2024-12-24 18:43:10.595|WRITE    |{mode -> Append, partitionBy -> []}                                                                                                                                                                                          |\n",
      "|9      |2024-12-24 18:42:54.971|WRITE    |{mode -> Overwrite, partitionBy -> []}                                                                                                                                                                                       |\n",
      "|8      |2024-12-24 18:07:40.88 |MERGE    |{predicate -> [\"((id#6580 = id#16881) AND (data_inversa#6581 = data_inversa#16882))\"], matchedPredicates -> [{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"actionType\":\"insert\"}], notMatchedBySourcePredicates -> []}|\n",
      "|7      |2024-12-24 18:06:30.044|WRITE    |{mode -> Append, partitionBy -> []}                                                                                                                                                                                          |\n",
      "|6      |2024-12-24 17:59:37.908|UPDATE   |{predicate -> [\"EndsWith(data_inversa#6581, /16)\"]}                                                                                                                                                                          |\n",
      "|5      |2024-12-24 17:58:26.316|WRITE    |{mode -> Append, partitionBy -> []}                                                                                                                                                                                          |\n",
      "|4      |2024-12-24 17:55:39.334|RESTORE  |{version -> 1, timestamp -> NULL}                                                                                                                                                                                            |\n",
      "|3      |2024-12-24 17:55:14.48 |RESTORE  |{version -> 0, timestamp -> NULL}                                                                                                                                                                                            |\n",
      "|2      |2024-12-24 17:41:11.685|WRITE    |{mode -> Append, partitionBy -> []}                                                                                                                                                                                          |\n",
      "+-------+-----------------------+---------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta_table.history().select(\"version\", \"timestamp\", \"operation\", \"operationParameters\").show(10, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in practice, no information is lost.\n",
    "\n",
    "Let's restore back to the version with the data from 2020 and 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/24 17:55:38 WARN DAGScheduler: Broadcasting large task binary with size 1079.8 KiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[table_size_after_restore: bigint, num_of_files_after_restore: bigint, num_removed_files: bigint, num_restored_files: bigint, removed_files_size: bigint, restored_files_size: bigint]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_table.restoreToVersion(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Update "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update operation is also done by the `DeltaTable` object. But we will use the `update` method with the SQL syntax."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's write the data from 2016 to the delta table. This data contains the \"data_inversa\" columns wrongly formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|data_inversa|\n",
      "+------------+\n",
      "|    10/06/16|\n",
      "|    01/01/16|\n",
      "|    01/01/16|\n",
      "|    01/01/16|\n",
      "|    01/01/16|\n",
      "+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_acidentes_2016 = (\n",
    "    spark\n",
    "    .read.format(\"csv\")\n",
    "    .option(\"delimiter\", \";\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"encoding\", \"ISO-8859-1\")\n",
    "    .schema(SCHEMA)\n",
    "    .load(\"data/acidentes/datatran2016.csv\")\n",
    ")\n",
    "\n",
    "df_acidentes_2016.select(\"data_inversa\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/24 17:58:25 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 25, schema size: 30\n",
      "CSV file: file:///Users/prabhakarapelluru/prabhakara/data_science/code/04-datascience-spark-python-2024/00-prabha-delta-lake-examples/01-delta-lake-fundamentals/data/acidentes/datatran2016.csv\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_acidentes_2016\\\n",
    "    .write.format(\"delta\")\\\n",
    "    .mode(\"append\")\\\n",
    "    .save(\"data_lake/delta/acidentes/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159948"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acidentes_latest.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acidentes_latest.createOrReplaceTempView(\"acidentes_latest\")\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    UPDATE acidentes_latest\n",
    "    SET data_inversa = CAST( TO_DATE(data_inversa, 'dd/MM/yy') AS STRING)\n",
    "    WHERE data_inversa LIKE '%/16'\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acidentes_latest.filter( F.col(\"data_inversa\").like(\"%/16\") ).count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Merge "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The merge operation, also known as upsert, is a mix of insert and update. It's also done by the `DeltaTable` object. But we will use the `merge` method with the SQL syntax."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simulate the utility of this method, we'll simulate a scenario where we have a data source that is constantly updating the data with new counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acidentes_2018 = (\n",
    "    spark\n",
    "    .read.format(\"csv\")\n",
    "    .option(\"delimiter\", \";\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"encoding\", \"ISO-8859-1\")\n",
    "    .schema(SCHEMA)\n",
    "    .load(\"data/acidentes/datatran2018.csv\")\n",
    ")\n",
    "\n",
    "df_acidentes_2018_zero = df_acidentes_2018.withColumn(\"pessoas\", F.lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acidentes_2018_zero\\\n",
    "    .write.format(\"delta\")\\\n",
    "    .mode(\"append\")\\\n",
    "    .save(\"data_lake/delta/acidentes/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "229281"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acidentes_latest.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+\n",
      "|data_inversa|pessoas|\n",
      "+------------+-------+\n",
      "|  2018-01-01|      0|\n",
      "|  2018-01-01|      0|\n",
      "|  2018-01-01|      0|\n",
      "|  2018-01-01|      0|\n",
      "|  2018-01-01|      0|\n",
      "+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_acidentes_latest.filter( F.col(\"data_inversa\").like(\"2018-%\") ).select([\"data_inversa\", \"pessoas\"]).show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's merge the data with the correct counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/24 18:07:40 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_updated_rows: bigint, num_deleted_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acidentes_latest.createOrReplaceTempView(\"acidentes_latest\")\n",
    "df_acidentes_2018.createOrReplaceTempView(\"acidentes_2018_new_counts\")\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    MERGE INTO acidentes_latest\n",
    "    USING acidentes_2018_new_counts\n",
    "\n",
    "    ON acidentes_latest.id = acidentes_2018_new_counts.id\n",
    "    AND acidentes_latest.data_inversa = acidentes_2018_new_counts.data_inversa\n",
    "    \n",
    "    WHEN MATCHED THEN\n",
    "        UPDATE SET pessoas = acidentes_latest.pessoas + acidentes_2018_new_counts.pessoas\n",
    "    \n",
    "    WHEN NOT MATCHED THEN\n",
    "        INSERT *\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+\n",
      "|data_inversa|pessoas|\n",
      "+------------+-------+\n",
      "|  2018-01-01|      2|\n",
      "|  2018-01-01|      4|\n",
      "|  2018-01-01|      2|\n",
      "|  2018-01-01|      2|\n",
      "|  2018-01-01|      1|\n",
      "+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_acidentes_latest.filter( F.col(\"data_inversa\").like(\"2018-%\") ).select([\"data_inversa\", \"pessoas\"]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
