{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52318ec2-d7be-4e3e-9d6b-8a0b12bc3c8a",
   "metadata": {},
   "source": [
    "# Delta Lake Basic Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a2f1591-02c1-4aab-9161-b8d476b58867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from delta import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2fcbebb-fe80-456a-9f1c-5ff7faa323de",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = (\n",
    "    pyspark.sql.SparkSession.builder.appName(\"MyApp\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "690fd8c6-de2b-46df-9cac-72a6e43002fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/24 14:34:19 WARN Utils: Your hostname, Prabhakaras-Mac-Book-Pro-2022.local resolves to a loopback address: 127.0.0.1; using 192.168.0.137 instead (on interface en0)\n",
      "24/12/24 14:34:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/prabhakarapelluru/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/prabhakarapelluru/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ebb44bcc-96a3-466f-9d69-5d7f78ea845f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.2.1 in central\n",
      "\tfound io.delta#delta-storage;3.2.1 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/anaconda3/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":: resolution report :: resolve 69ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.2.1 from central in [default]\n",
      "\tio.delta#delta-storage;3.2.1 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ebb44bcc-96a3-466f-9d69-5d7f78ea845f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/2ms)\n",
      "24/12/24 14:34:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/24 14:34:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/12/24 14:34:20 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/12/24 14:34:20 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c2c51a-7e0e-4161-8e5d-78d1aba58c68",
   "metadata": {},
   "source": [
    "## Create Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a379483-34b7-42ba-86ca-325e8a6b0534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "  CREATE TABLE fun_delta (id INT, data STRING) USING delta\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c334d07f-bff8-47dc-aa2f-45796ea9050d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"ALTER TABLE `fun_delta` SET TBLPROPERTIES (\n",
    "   'delta.columnMapping.mode' = 'name',\n",
    "   'delta.minReaderVersion' = '2',\n",
    "   'delta.minWriterVersion' = '5')\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0826410-fe8a-4f91-90c8-c03bc40121b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"INSERT INTO fun_delta VALUES (1, 'a'), (2, 'b'), (3, 'c')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d34c792-2809-4a61-ad7c-bcbf749ccc14",
   "metadata": {},
   "source": [
    "## Append rows to existing table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9d2eeb0-c994-4518-932d-57ed3cdaa8ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"INSERT INTO fun_delta VALUES (4, 'd')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6843e261-2b08-4a02-8e8f-aacc585d87cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/24 14:35:19 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|data|\n",
      "+---+----+\n",
      "|  2|   b|\n",
      "|  4|   d|\n",
      "|  3|   c|\n",
      "|  1|   a|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from fun_delta\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79189d8-f52e-4291-a5bb-0cc1837116aa",
   "metadata": {},
   "source": [
    "## Alter table ADD COLUMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f718c8b6-3820-48b7-9c92-7aa2b90fa52c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "ALTER TABLE fun_delta\n",
    "ADD COLUMNS (\n",
    "    my_new_column string\n",
    "  )\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4cc6fece-6f1d-47fe-9dd1-1ac90eec112d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------------+\n",
      "| id|data|my_new_column|\n",
      "+---+----+-------------+\n",
      "|  2|   b|         NULL|\n",
      "|  4|   d|         NULL|\n",
      "|  3|   c|         NULL|\n",
      "|  1|   a|         NULL|\n",
      "+---+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from fun_delta\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dd37c2-c3f5-4bd2-82a4-99ed53670300",
   "metadata": {},
   "source": [
    "## Alter table RENAME COLUMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6b68e54-7c51-4d6a-ae64-39efab25780d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "ALTER TABLE fun_delta RENAME COLUMN data TO letter\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6dc3975c-1a1e-4885-a183-4b6524c0052d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------------+\n",
      "| id|letter|my_new_column|\n",
      "+---+------+-------------+\n",
      "|  2|     b|         NULL|\n",
      "|  4|     d|         NULL|\n",
      "|  3|     c|         NULL|\n",
      "|  1|     a|         NULL|\n",
      "+---+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from fun_delta\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5210ec8-4d1f-499a-b800-d7734711987c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1123f1-f178-4209-8380-fc9f86650fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
