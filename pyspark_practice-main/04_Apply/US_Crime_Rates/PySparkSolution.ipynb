{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# United States - Crime Rates - 1960 - 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction:\n",
    "\n",
    "This time you will create a data \n",
    "\n",
    "Special thanks to: https://github.com/justmarkham for sharing the dataset and materials.\n",
    "\n",
    "### Step 1. Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.20:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>crime</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x11833e430>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"crime\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Import the dataset from this [address](https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/04_Apply/US_Crime_Rates/US_Crime_Rates_1960_2014.csv). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Assign it to a variable called crime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-------+-------+--------+------+-------------+-------+------------------+--------+-------------+-------------+\n",
      "|Year|Population|  Total|Violent|Property|Murder|Forcible_Rape|Robbery|Aggravated_assault|Burglary|Larceny_Theft|Vehicle_Theft|\n",
      "+----+----------+-------+-------+--------+------+-------------+-------+------------------+--------+-------------+-------------+\n",
      "|1960| 179323175|3384200| 288460| 3095700|  9110|        17190| 107840|            154320|  912100|      1855400|       328200|\n",
      "|1961| 182992000|3488000| 289390| 3198600|  8740|        17220| 106670|            156760|  949600|      1913000|       336000|\n",
      "|1962| 185771000|3752200| 301510| 3450700|  8530|        17550| 110860|            164570|  994300|      2089600|       366800|\n",
      "|1963| 188483000|4109500| 316970| 3792500|  8640|        17650| 116470|            174210| 1086400|      2297800|       408300|\n",
      "|1964| 191141000|4564600| 364220| 4200400|  9360|        21420| 130390|            203050| 1213200|      2514400|       472800|\n",
      "+----+----------+-------+-------+--------+------+-------------+-------+------------------+--------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/04_Apply/US_Crime_Rates/US_Crime_Rates_1960_2014.csv\"\n",
    "\n",
    "spark.sparkContext.addFile(url)\n",
    "\n",
    "crime = spark.read.csv(SparkFiles.get(\"US_Crime_Rates_1960_2014.csv\"), inferSchema=True, header=True, sep=\",\" )\n",
    "crime.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. What is the type of the columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Population: integer (nullable = true)\n",
      " |-- Total: integer (nullable = true)\n",
      " |-- Violent: integer (nullable = true)\n",
      " |-- Property: integer (nullable = true)\n",
      " |-- Murder: integer (nullable = true)\n",
      " |-- Forcible_Rape: integer (nullable = true)\n",
      " |-- Robbery: integer (nullable = true)\n",
      " |-- Aggravated_assault: integer (nullable = true)\n",
      " |-- Burglary: integer (nullable = true)\n",
      " |-- Larceny_Theft: integer (nullable = true)\n",
      " |-- Vehicle_Theft: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crime.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Have you noticed that the type of Year is int64. But pandas has a different type to work with Time Series. Let's see it now.\n",
    "\n",
    "### Step 5. Convert the type of the column Year to datetime64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: date (nullable = true)\n",
      " |-- Population: integer (nullable = true)\n",
      " |-- Total: integer (nullable = true)\n",
      " |-- Violent: integer (nullable = true)\n",
      " |-- Property: integer (nullable = true)\n",
      " |-- Murder: integer (nullable = true)\n",
      " |-- Forcible_Rape: integer (nullable = true)\n",
      " |-- Robbery: integer (nullable = true)\n",
      " |-- Aggravated_assault: integer (nullable = true)\n",
      " |-- Burglary: integer (nullable = true)\n",
      " |-- Larceny_Theft: integer (nullable = true)\n",
      " |-- Vehicle_Theft: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#different solutions from internet\n",
    "\n",
    "# import pyspark.sql.functions as f\n",
    "# df.withColumn(\"txn_dt\", f.to_date(f.from_unixtime(f.col(\"txn_dt\"))))\n",
    "\n",
    "# df_2.select(\"start_dt\",\"end_dt\",date_format(\"start_dt\",'dd/MM/yyyy').alias(\"dt_format\")).show()\n",
    "\n",
    "# data = data.withColumn('Year', year(col('Date')))\n",
    "\n",
    "crime = crime.withColumn(\"Year\", f.to_date(f.from_unixtime(f.col(\"Year\"))))\n",
    "crime.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      Year|\n",
      "+----------+\n",
      "|1970-01-01|\n",
      "|1970-01-01|\n",
      "|1970-01-01|\n",
      "|1970-01-01|\n",
      "|1970-01-01|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crime.select(\"Year\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6. Set the Year column as the index of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying to find an answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7. Delete the Total column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+--------+------+-------------+-------+------------------+--------+-------------+-------------+\n",
      "|      Year|Population|Violent|Property|Murder|Forcible_Rape|Robbery|Aggravated_assault|Burglary|Larceny_Theft|Vehicle_Theft|\n",
      "+----------+----------+-------+--------+------+-------------+-------+------------------+--------+-------------+-------------+\n",
      "|1970-01-01| 179323175| 288460| 3095700|  9110|        17190| 107840|            154320|  912100|      1855400|       328200|\n",
      "|1970-01-01| 182992000| 289390| 3198600|  8740|        17220| 106670|            156760|  949600|      1913000|       336000|\n",
      "|1970-01-01| 185771000| 301510| 3450700|  8530|        17550| 110860|            164570|  994300|      2089600|       366800|\n",
      "|1970-01-01| 188483000| 316970| 3792500|  8640|        17650| 116470|            174210| 1086400|      2297800|       408300|\n",
      "|1970-01-01| 191141000| 364220| 4200400|  9360|        21420| 130390|            203050| 1213200|      2514400|       472800|\n",
      "+----------+----------+-------+--------+------+-------------+-------+------------------+--------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df.drop(\"firstname\") \\\n",
    "#   .printSchema()\n",
    "# \"\"\" import col is required \"\"\"  \n",
    "# df.drop(col(\"firstname\")) \\\n",
    "#   .printSchema()  \n",
    "  \n",
    "# df.drop(df.firstname) \\\n",
    "#   .printSchema()\n",
    "\n",
    "crime.drop(\"Total\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8. Group the year by decades and sum the values\n",
    "\n",
    "#### Pay attention to the Population column number, summing this column is a mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#found no satisfactoey answer\n",
    "# https://stackoverflow.com/questions/39271374/pyspark-how-to-resample-frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9. What is the most dangerous decade to live in the US?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Year',\n",
       " 'Population',\n",
       " 'Total',\n",
       " 'Violent',\n",
       " 'Property',\n",
       " 'Murder',\n",
       " 'Forcible_Rape',\n",
       " 'Robbery',\n",
       " 'Aggravated_assault',\n",
       " 'Burglary',\n",
       " 'Larceny_Theft',\n",
       " 'Vehicle_Theft']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crime.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-------+-------+--------+------+-------------+-------+------------------+--------+-------------+-------------+\n",
      "|Year|Population|  Total|Violent|Property|Murder|Forcible_Rape|Robbery|Aggravated_assault|Burglary|Larceny_Theft|Vehicle_Theft|\n",
      "+----+----------+-------+-------+--------+------+-------------+-------+------------------+--------+-------------+-------------+\n",
      "|1960| 179323175|3384200| 288460| 3095700|  9110|        17190| 107840|            154320|  912100|      1855400|       328200|\n",
      "|1961| 182992000|3488000| 289390| 3198600|  8740|        17220| 106670|            156760|  949600|      1913000|       336000|\n",
      "|1962| 185771000|3752200| 301510| 3450700|  8530|        17550| 110860|            164570|  994300|      2089600|       366800|\n",
      "|1963| 188483000|4109500| 316970| 3792500|  8640|        17650| 116470|            174210| 1086400|      2297800|       408300|\n",
      "|1964| 191141000|4564600| 364220| 4200400|  9360|        21420| 130390|            203050| 1213200|      2514400|       472800|\n",
      "+----+----------+-------+-------+--------+------+-------------+-------+------------------+--------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/04_Apply/US_Crime_Rates/US_Crime_Rates_1960_2014.csv\"\n",
    "\n",
    "spark.sparkContext.addFile(url)\n",
    "\n",
    "crime = spark.read.csv(SparkFiles.get(\"US_Crime_Rates_1960_2014.csv\"), inferSchema=True, header=True, sep=\",\" )\n",
    "crime.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year : 2014\n",
      "Population : 2014\n",
      "Total : 1991\n",
      "Violent : 1992\n",
      "Property : 1991\n",
      "Murder : 1991\n",
      "Forcible_Rape : 1992\n",
      "Robbery : 1991\n",
      "Aggravated_assault : 1993\n",
      "Burglary : 1980\n",
      "Larceny_Theft : 1991\n",
      "Vehicle_Theft : 1991\n"
     ]
    }
   ],
   "source": [
    "for cols in crime.columns:\n",
    "    row1 = crime.agg({cols: \"max\"}).head(1)[0][0]\n",
    "    year = crime.where(crime[cols].isin(row1)).head(1)[0][0]\n",
    "    print(cols,\":\",year)\n",
    "#     print(cols,\":\", crime.orderBy(desc(cols)).head(1)[0][0])\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
